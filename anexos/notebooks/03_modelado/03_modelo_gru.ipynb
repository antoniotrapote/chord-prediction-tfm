{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e4e389d",
   "metadata": {},
   "source": [
    "**Nota**: Se recomienda ejecutar estos notebooks en Google Colab para asegurar la compatibilidad y evitar problemas de dependencias. El entrenamiento de los modelos requiere una cantidad significativa de memoria RAM y potencia de c√≥mputo, que puede no estar disponible en todos los entornos locales.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/antoniotrapote/chord-prediction-tfm/blob/main/anexos/notebooks/03_modelado/03_modelo_gru.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-black?logo=github)](https://github.com/antoniotrapote/chord-prediction-tfm/blob/main/anexos/notebooks/03_modelado/03_modelo_gru.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46a9d23",
   "metadata": {
    "id": "c46a9d23"
   },
   "source": [
    "# Gated Recurrent Unit (GRU) model - Pytorch\n",
    "\n",
    "Hemos utilizado PyTorch para implementar y entrenar un modelo de red neuronal recurrente basado en Gated Recurrent Units (GRU) para la predicci√≥n de acordes en secuencias musicales.\n",
    "\n",
    "El √∫ltimo dataset utilizado fue `songdb_funcional_v4`\n",
    "\n",
    "Contenido del notebook:\n",
    "1. Entorno (Colab) - comprobaci√≥n de versiones\n",
    "2. Configuraci√≥n de par√°metros\n",
    "3. Traer el CSV a Colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6c127d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce68bab",
   "metadata": {
    "id": "5ce68bab"
   },
   "source": [
    "## 1) Entorno (Colab)\n",
    "El modelo fue entrenado en Google Colab, con las siguientes especificaciones:\n",
    ">Python: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]  \n",
    ">PyTorch: 2.8.0+cu126  \n",
    ">CUDA disponible: True  \n",
    ">GPU: Tesla T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c816e35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6192,
     "status": "ok",
     "timestamp": 1756230420307,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "4c816e35",
    "outputId": "7689db14-f9d9-4f17-b7c8-e62343ec5594"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
      "PyTorch: 2.8.0+cu126\n",
      "CUDA disponible: True\n",
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "#@title Comprobar GPU/versions\n",
    "import sys, torch\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Activa GPU: Runtime ‚ñ∂ Change runtime type ‚ñ∂ GPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a81959",
   "metadata": {
    "id": "73a81959"
   },
   "source": [
    "## 2) Configuraci√≥n de par√°metros y rutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5769e658",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1756230420345,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "5769e658",
    "outputId": "e7378c08-d807-4c9c-857d-e6a0003b65ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(data_path='https://raw.githubusercontent.com/antoniotrapote/chord-prediction-tfm/main/anexos/datasets/songdb_funcional_v4.csv', sequence_col='funcional_prog', val_size=0.1, test_size=0.1, random_state=42, seq_len=24, batch_size=128, epochs=6, lr=0.002, weight_decay=0.0001, dropout=0.2, embedding_dim=128, hidden_size=256, num_layers=2, grad_clip=1.0, amp=True, save_dir='/content/models_gru_v1', save_name='gru_best.pt', tokenizer_path='gru_tokenizer.json', min_seq_len=8)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Ruta al CSV en Colab (sube el archivo o m√≥ntalo desde Drive)\n",
    "    data_path: str = \"/content/songdb_funcional_v4.csv\"\n",
    "    # üîß Elige la columna de secuencia (sin autodetecci√≥n):\n",
    "    sequence_col: str = \"funcional_prog\"\n",
    "\n",
    "    # Splits y semilla\n",
    "    val_size: float = 0.10\n",
    "    test_size: float = 0.10\n",
    "    random_state: int = 42\n",
    "\n",
    "    # Modelo/entrenamiento\n",
    "    seq_len: int = 24\n",
    "    batch_size: int = 128\n",
    "    epochs: int = 6\n",
    "    lr: float = 2e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    dropout: float = 0.2\n",
    "    embedding_dim: int = 128\n",
    "    hidden_size: int = 256\n",
    "    num_layers: int = 2\n",
    "    grad_clip: float = 1.0\n",
    "    amp: bool = True\n",
    "\n",
    "    # Guardado\n",
    "    save_dir: str = \"/content/models_gru_v1\"\n",
    "    save_name: str = \"gru_best.pt\"\n",
    "    tokenizer_path: str = \"gru_tokenizer.json\"\n",
    "\n",
    "    # Filtrado\n",
    "    min_seq_len: int = 8  # ignora secuencias muy cortas\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dbc89e",
   "metadata": {
    "id": "e9dbc89e"
   },
   "source": [
    "## 2) Traer el CSV a Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45857b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "# Subir el dataset desde GitHub\n",
    "# Configuraci√≥n para acceder al dataset `songdb_funcional_v4.csv` en GitHub\n",
    "USER = \"antoniotrapote\"\n",
    "REPO = \"chord-prediction-tfm\"\n",
    "BRANCH = \"main\"\n",
    "PATH_IN_REPO = \"anexos/data/songdb_funcional_v4.csv\"\n",
    "URL = f\"https://raw.githubusercontent.com/{USER}/{REPO}/{BRANCH}/{PATH_IN_REPO}\"\n",
    "\n",
    "# Descargar el archivo CSV desde GitHub\n",
    "urllib.request.urlretrieve(URL, cfg.data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2afd05",
   "metadata": {
    "id": "8f2afd05"
   },
   "source": [
    "## 3) Cargar CSV y tokenizar (whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6916af6e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "executionInfo": {
     "elapsed": 495,
     "status": "ok",
     "timestamp": 1756230452250,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "6916af6e",
    "outputId": "5bdb6403-1616-48bb-f1f8-c5c2315304bb"
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#@title Carga + tokenizaci√≥n\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mast\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m cfg.sequence_col \u001b[38;5;129;01min\u001b[39;00m df.columns, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumna \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg.sequence_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m no encontrada en el CSV.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFilas totales:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/VSCode/TFM/chord-prediction-tfm/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/VSCode/TFM/chord-prediction-tfm/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/VSCode/TFM/chord-prediction-tfm/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/VSCode/TFM/chord-prediction-tfm/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/VSCode/TFM/chord-prediction-tfm/.venv/lib/python3.12/site-packages/pandas/io/common.py:728\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    725\u001b[39m     codecs.lookup_error(errors)\n\u001b[32m    727\u001b[39m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m ioargs = \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    736\u001b[39m handle = ioargs.filepath_or_buffer\n\u001b[32m    737\u001b[39m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/VSCode/TFM/chord-prediction-tfm/.venv/lib/python3.12/site-packages/pandas/io/common.py:384\u001b[39m, in \u001b[36m_get_filepath_or_buffer\u001b[39m\u001b[34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[32m    383\u001b[39m req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[32m    385\u001b[39m     content_encoding = req.headers.get(\u001b[33m\"\u001b[39m\u001b[33mContent-Encoding\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding == \u001b[33m\"\u001b[39m\u001b[33mgzip\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/VSCode/TFM/chord-prediction-tfm/.venv/lib/python3.12/site-packages/pandas/io/common.py:289\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    284\u001b[39m \u001b[33;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03mthe stdlib.\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrequest\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:215\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    214\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:521\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    519\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_response.get(protocol, []):\n\u001b[32m    520\u001b[39m     meth = \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m     response = \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:630\u001b[39m, in \u001b[36mHTTPErrorProcessor.http_response\u001b[39m\u001b[34m(self, request, response)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[32m    628\u001b[39m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[32m200\u001b[39m <= code < \u001b[32m300\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:559\u001b[39m, in \u001b[36mOpenerDirector.error\u001b[39m\u001b[34m(self, proto, *args)\u001b[39m\n\u001b[32m    557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[32m    558\u001b[39m     args = (\u001b[38;5;28mdict\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhttp_error_default\u001b[39m\u001b[33m'\u001b[39m) + orig_args\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:492\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    491\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    494\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:639\u001b[39m, in \u001b[36mHTTPDefaultErrorHandler.http_error_default\u001b[39m\u001b[34m(self, req, fp, code, msg, hdrs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[32m--> \u001b[39m\u001b[32m639\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001b[31mHTTPError\u001b[39m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "#@title Carga + tokenizaci√≥n\n",
    "import pandas as pd, ast, re\n",
    "\n",
    "df = pd.read_csv(cfg.data_path)\n",
    "assert cfg.sequence_col in df.columns, f\"Columna {cfg.sequence_col} no encontrada en el CSV.\"\n",
    "print(\"Filas totales:\", len(df))\n",
    "display(df[[cfg.sequence_col]].head(3))\n",
    "\n",
    "def parse_tokens_simple(s: str):\n",
    "    if isinstance(s, str) and s.strip().startswith(\"[\") and s.strip().endswith(\"]\"):\n",
    "        try:\n",
    "            lst = ast.literal_eval(s)\n",
    "            if isinstance(lst, list):\n",
    "                return [str(t) for t in lst]\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Normaliza separadores de comp√°s y saltos de l√≠nea a espacios\n",
    "    s = str(s).replace(\"|\", \" \").replace(\"\\n\", \" \")#.replace(\" \", \" \")\n",
    "    toks = [t for t in re.findall(r\"\\S+\", s) if t.strip()]\n",
    "    return toks\n",
    "\n",
    "df[\"_tokens_\"] = df[cfg.sequence_col].apply(parse_tokens_simple)\n",
    "df = df[df[\"_tokens_\"].apply(len) >= cfg.min_seq_len].reset_index(drop=True)\n",
    "print(\"Filas tras filtro min_seq_len:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6efa6d6",
   "metadata": {
    "id": "a6efa6d6"
   },
   "source": [
    "## 4) Split train/val/test (simple, por filas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6426eba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1018,
     "status": "ok",
     "timestamp": 1756230453270,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "f6426eba",
    "outputId": "57a49212-c1c4-49ff-f5e8-03a4f70a0e01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2089 261 262\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#@title Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, tmp_df = train_test_split(df, test_size=cfg.val_size+cfg.test_size, random_state=cfg.random_state, shuffle=True)\n",
    "rel_test = cfg.test_size / (cfg.val_size + cfg.test_size) if (cfg.val_size + cfg.test_size) > 0 else 0.5\n",
    "val_df, test_df = train_test_split(tmp_df, test_size=rel_test, random_state=cfg.random_state, shuffle=True)\n",
    "\n",
    "train_seqs = train_df[\"_tokens_\"].tolist()\n",
    "val_seqs   = val_df[\"_tokens_\"].tolist()\n",
    "test_seqs  = test_df[\"_tokens_\"].tolist()\n",
    "print(len(train_seqs), len(val_seqs), len(test_seqs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8622e1a7",
   "metadata": {
    "id": "8622e1a7"
   },
   "source": [
    "## 5) Vocabulario y codificaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39588251",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1756230453278,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "39588251",
    "outputId": "d7919cf1-425b-44a8-97ff-01e4c174d9d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 86\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#@title Vocab + encode\n",
    "from collections import Counter\n",
    "import json\n",
    "PAD, UNK, BOS, EOS = \"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"\n",
    "\n",
    "def build_vocab(seqs, min_freq=1):\n",
    "    c = Counter()\n",
    "    for s in seqs: c.update(s)\n",
    "    vocab = [PAD, UNK, BOS, EOS] + [t for t,f in c.items() if f >= min_freq and t not in {PAD,UNK,BOS,EOS}]\n",
    "    stoi = {t:i for i,t in enumerate(vocab)}\n",
    "    itos = {i:t for t,i in stoi.items()}\n",
    "    return vocab, stoi, itos\n",
    "\n",
    "vocab, stoi, itos = build_vocab(train_seqs, 1)\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "with open(cfg.tokenizer_path, \"w\") as f:\n",
    "    json.dump({\"vocab\": vocab}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def encode(seq, add_bos=True):\n",
    "    ids = [stoi[BOS]] if add_bos else []\n",
    "    ids += [stoi.get(t, stoi[UNK]) for t in seq]\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a2c319",
   "metadata": {
    "id": "30a2c319"
   },
   "source": [
    "## 6) Dataset (context‚Üínext) y DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "508cd745",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 196,
     "status": "ok",
     "timestamp": 1756230453475,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "508cd745",
    "outputId": "a65cfd85-5a70-4997-a6eb-394306d2778b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60979, 7044, 7669)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#@title Dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class NextTokenDataset(Dataset):\n",
    "    def __init__(self, sequences, seq_len):\n",
    "        self.samples = []\n",
    "        for seq in sequences:\n",
    "            ids = encode(seq, add_bos=True)\n",
    "            if len(ids) <= seq_len: continue\n",
    "            for i in range(seq_len, len(ids)):\n",
    "                self.samples.append((ids[i-seq_len:i], ids[i]))\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.samples[idx]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "train_data = NextTokenDataset(train_seqs, cfg.seq_len)\n",
    "val_data   = NextTokenDataset(val_seqs,   cfg.seq_len)\n",
    "test_data  = NextTokenDataset(test_seqs,  cfg.seq_len)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=cfg.batch_size, shuffle=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_data,   batch_size=cfg.batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_data,  batch_size=cfg.batch_size, shuffle=False)\n",
    "\n",
    "len(train_data), len(val_data), len(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfd5605",
   "metadata": {
    "id": "abfd5605"
   },
   "source": [
    "## 7) Modelo GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fe182f4",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1756230453479,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "0fe182f4"
   },
   "outputs": [],
   "source": [
    "\n",
    "#@title Definici√≥n del modelo\n",
    "import torch.nn as nn\n",
    "class ChordGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_size, num_layers=num_layers,\n",
    "                                   batch_first=True, dropout=dropout if num_layers>1 else 0.0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    def forward(self, x):\n",
    "        e = self.emb(x)\n",
    "        o, _ = self.rnn(e)\n",
    "        return self.fc(self.dropout(o[:, -1, :]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a83d383",
   "metadata": {
    "id": "8a83d383"
   },
   "source": [
    "## 8) Entrenamiento y m√©tricas (Top@K, MRR, PPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66f6479c",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1756230453495,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "66f6479c"
   },
   "outputs": [],
   "source": [
    "\n",
    "#@title Utils\n",
    "import math, time, os, torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def topk_metrics(logits, targets, ks=(1,3,5)):\n",
    "    out = {}\n",
    "    with torch.no_grad():\n",
    "        for k in ks:\n",
    "            topk = logits.topk(k, dim=-1).indices\n",
    "            out[f\"Top@{k}\"] = (topk == targets.unsqueeze(1)).any(dim=1).float().mean().item()\n",
    "        ranks = (logits.argsort(dim=-1, descending=True) == targets.unsqueeze(1)).nonzero(as_tuple=False)[:,1] + 1\n",
    "        out[\"MRR\"] = (1.0 / ranks.float()).mean().item()\n",
    "    return out\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total, n = 0.0, 0\n",
    "    agg = {\"Top@1\":0.0,\"Top@3\":0.0,\"Top@5\":0.0,\"MRR\":0.0}\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            b = x.size(0); total += loss.item()*b; n += b\n",
    "            m = topk_metrics(logits, y)\n",
    "            for k in agg: agg[k] += m[k]*b\n",
    "    for k in agg: agg[k] /= max(1,n)\n",
    "    return {\"loss\": total/max(1,n), \"ppl\": math.exp(total/max(1,n)), **agg}\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, lr, weight_decay, grad_clip=1.0, amp=True, save_dir=\".\", save_name=\"best.pt\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(amp and device.type=='cuda'))\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    crit = torch.nn.CrossEntropyLoss()\n",
    "    best_mrr, best_path = -1.0, os.path.join(save_dir, save_name)\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train(); t0 = time.time()\n",
    "        for i,(x,y) in enumerate(train_loader,1):\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=(amp and device.type=='cuda')):\n",
    "                logits = model(x); loss = crit(logits,y)\n",
    "            scaler.scale(loss).backward()\n",
    "            if grad_clip is not None:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            scaler.step(opt); scaler.update()\n",
    "            if i % 100 == 0: print(f\"Ep{ep} step {i}/{len(train_loader)} loss {loss.item():.4f}\")\n",
    "        valm = evaluate(model, val_loader, crit)\n",
    "        print(f\"Epoch {ep} | val loss {valm['loss']:.4f} ppl {valm['ppl']:.2f} Top@1 {valm['Top@1']:.3f} Top@3 {valm['Top@3']:.3f} Top@5 {valm['Top@5']:.3f} MRR {valm['MRR']:.3f}\")\n",
    "        if valm[\"MRR\"] > best_mrr:\n",
    "            best_mrr = valm[\"MRR\"]\n",
    "            torch.save({\"model_state\": model.state_dict(), \"config\": dict(vars(cfg)), \"stoi\": stoi, \"itos\": itos}, best_path)\n",
    "            print(\"üî• Guardado best ->\", best_path, \"| MRR:\", best_mrr)\n",
    "    return best_mrr, best_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd7430c",
   "metadata": {
    "id": "6cd7430c"
   },
   "source": [
    "## 9) Entrenar GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "464a26fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29795,
     "status": "ok",
     "timestamp": 1756230483303,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "464a26fd",
    "outputId": "5a48f4d0-787b-4f2e-ebc2-3cd8bbb15541"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-1507830936.py:32: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(amp and device.type=='cuda'))\n",
      "/tmp/ipython-input-1507830936.py:41: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(amp and device.type=='cuda')):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep1 step 100/476 loss 2.2282\n",
      "Ep1 step 200/476 loss 2.3747\n",
      "Ep1 step 300/476 loss 2.0149\n",
      "Ep1 step 400/476 loss 2.0779\n",
      "Epoch 1 | val loss 2.1584 ppl 8.66 Top@1 0.442 Top@3 0.676 Top@5 0.763 MRR 0.585\n",
      "üî• Guardado best -> /content/models_gru_v1/gru_best.pt | MRR: 0.584794789535765\n",
      "Ep2 step 100/476 loss 2.1084\n",
      "Ep2 step 200/476 loss 2.1919\n",
      "Ep2 step 300/476 loss 2.1190\n",
      "Ep2 step 400/476 loss 2.1181\n",
      "Epoch 2 | val loss 2.1306 ppl 8.42 Top@1 0.448 Top@3 0.685 Top@5 0.768 MRR 0.591\n",
      "üî• Guardado best -> /content/models_gru_v1/gru_best.pt | MRR: 0.591246411382035\n",
      "Ep3 step 100/476 loss 2.0119\n",
      "Ep3 step 200/476 loss 1.8909\n",
      "Ep3 step 300/476 loss 1.8901\n",
      "Ep3 step 400/476 loss 2.1402\n",
      "Epoch 3 | val loss 2.1259 ppl 8.38 Top@1 0.455 Top@3 0.688 Top@5 0.770 MRR 0.597\n",
      "üî• Guardado best -> /content/models_gru_v1/gru_best.pt | MRR: 0.5970695267882555\n",
      "Ep4 step 100/476 loss 1.9535\n",
      "Ep4 step 200/476 loss 1.8190\n",
      "Ep4 step 300/476 loss 1.8945\n",
      "Ep4 step 400/476 loss 2.0423\n",
      "Epoch 4 | val loss 2.1204 ppl 8.33 Top@1 0.459 Top@3 0.687 Top@5 0.770 MRR 0.598\n",
      "üî• Guardado best -> /content/models_gru_v1/gru_best.pt | MRR: 0.59776738731107\n",
      "Ep5 step 100/476 loss 1.9358\n",
      "Ep5 step 200/476 loss 1.9275\n",
      "Ep5 step 300/476 loss 1.6719\n",
      "Ep5 step 400/476 loss 1.6908\n",
      "Epoch 5 | val loss 2.1479 ppl 8.57 Top@1 0.457 Top@3 0.688 Top@5 0.771 MRR 0.597\n",
      "Ep6 step 100/476 loss 1.7920\n",
      "Ep6 step 200/476 loss 1.8874\n",
      "Ep6 step 300/476 loss 1.6937\n",
      "Ep6 step 400/476 loss 2.0884\n",
      "Epoch 6 | val loss 2.1510 ppl 8.59 Top@1 0.457 Top@3 0.688 Top@5 0.771 MRR 0.597\n",
      "Best MRR: 0.59776738731107 | path: /content/models_gru_v1/gru_best.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#@title Train\n",
    "import torch, os, json\n",
    "torch.manual_seed(cfg.random_state)\n",
    "model = ChordGRU(vocab_size=len(vocab), embedding_dim=cfg.embedding_dim,\n",
    "                          hidden_size=cfg.hidden_size, num_layers=cfg.num_layers,\n",
    "                          dropout=cfg.dropout).to(device)\n",
    "best_mrr, best_path = train_model(model, train_loader, val_loader, cfg.epochs, cfg.lr, cfg.weight_decay,\n",
    "                                  grad_clip=cfg.grad_clip, amp=cfg.amp, save_dir=cfg.save_dir, save_name=cfg.save_name)\n",
    "print(\"Best MRR:\", best_mrr, \"| path:\", best_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788c8524",
   "metadata": {
    "id": "788c8524"
   },
   "source": [
    "## 10) Evaluaci√≥n en Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cd6eb80",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1756230483594,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "2cd6eb80",
    "outputId": "c10def60-209e-4a0f-92a7-c14997ef504c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: {'loss': 2.1969814152866194, 'ppl': 8.997811807607055, 'Top@1': 0.4319989573755014, 'Top@3': 0.6775329250729151, 'Top@5': 0.7616377626377533, 'MRR': 0.5797308985110382}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#@title Test\n",
    "import torch, os\n",
    "ckpt = torch.load(os.path.join(cfg.save_dir, cfg.save_name), map_location=device)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "test_metrics = evaluate(model, test_loader, torch.nn.CrossEntropyLoss())\n",
    "print(\"Test:\", test_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ce4cb8",
   "metadata": {
    "id": "42ce4cb8"
   },
   "source": [
    "## 11) predict_next(context, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4c75249",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1756230483595,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "d4c75249",
    "outputId": "80e1967d-d9f0-4c8a-d6c7-72b353402dc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: ['III', 'III', '#IV', '#IV', 'bII', 'bII', 'natIII', 'natIII', 'III', '#IV', 'IV', 'VI', 'V', 'bVII', 'natVI', 'I', 'II', 'VII', 'VI', 'IV', 'III', 'I', 'VII', 'natVI']\n",
      "Pred: [('VII', 0.13458140194416046), ('VI', 0.12113039195537567), ('I', 0.11372911930084229), ('IV', 0.08486910164356232), ('III', 0.07772155851125717)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#@title predict_next()\n",
    "import torch.nn.functional as F\n",
    "def predict_next(context_tokens, k=5):\n",
    "    model.eval()\n",
    "    ids = [stoi.get(t, stoi[\"<unk>\"]) for t in context_tokens]\n",
    "    if len(ids) < cfg.seq_len:\n",
    "        ids = [stoi[\"<bos>\"]] * (cfg.seq_len - len(ids)) + ids\n",
    "    else:\n",
    "        ids = ids[-cfg.seq_len:]\n",
    "    import torch\n",
    "    x = torch.tensor(ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits[0], dim=-1)\n",
    "        topk = torch.topk(probs, k)\n",
    "        return [(itos[i], float(p)) for i,p in zip(topk.indices.tolist(), topk.values.tolist())]\n",
    "\n",
    "# Ejemplo r√°pido (si hay datos)\n",
    "if len(train_seqs):\n",
    "    ctx = train_seqs[0][:cfg.seq_len]\n",
    "    print(\"Context:\", ctx)\n",
    "    print(\"Pred:\", predict_next(ctx, k=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24124ee2",
   "metadata": {
    "id": "24124ee2"
   },
   "source": [
    "\n",
    "### Roadmap corto\n",
    "- A√±adir **posici√≥n en comp√°s** y **duraci√≥n** como embeddings adicionales (v2).\n",
    "- Re-ranking suave para evitar repes y favorecer cadencias.\n",
    "- Ajustar `seq_len`, capas y *scheduler* cuando confirmes el pipeline.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
