{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1065c86",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/antoniotrapote/chord-prediction-tfm/blob/main/anexos/notebooks/03_modelado/01_modelos_kn_hmm.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-black?logo=github)](https://github.com/antoniotrapote/chord-prediction-tfm/blob/main/anexos/notebooks/03_modelado/01_modelos_kn_hmm.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a8d5a",
   "metadata": {},
   "source": [
    "# Modelos tradicionales: Kneser–Ney y HMM (T/S/D)\n",
    "Este notebook implementa y evalúa dos modelos tradicionales para predicción de acordes:\n",
    "- **Modelo n-grama con Kneser–Ney (trigrama)** sobre tokens funcionales (e.g., `I`, `ii`, `V7`, `bVII7`), con `<unk>` para raros y límites `<s>`, `</s>`.\n",
    "- **HMM de funciones (T/S/D)** con estados ocultos **Tónica (T)**, **Subdominante (S)** y **Dominante (D)** derivados heurísticamente de la etiqueta funcional; emisiones son **tokens funcionales**.\n",
    "\n",
    "**dataset**: `songdb_funcional_v4.csv`\n",
    "\n",
    "**Contenido del notebook**:\n",
    "1. Carga del dataset y preparación de datos.\n",
    "2. Partición en train/val/test.\n",
    "3. Implementación y ajuste del modelo n-grama con Kneser–Ney\n",
    "4. Implementación y ajuste del HMM supervisado (T/S/D).\n",
    "5. Evaluación comparativa de ambos modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8366f8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53882b3",
   "metadata": {},
   "source": [
    "## 1) Cargamos el dataset\n",
    "Leemos el CSV y preparamos las secuencias de tokens por canción. Usamos `title` + `composedby` como ID de canción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72d5df93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración para acceder al dataset `songdb_funcional_v4.csv` en GitHub\n",
    "USER = \"antoniotrapote\"\n",
    "REPO = \"chord-prediction-tfm\"\n",
    "BRANCH = \"main\"\n",
    "PATH_IN_REPO = \"anexos/data/songdb_funcional_v4.csv\"\n",
    "URL = f\"https://raw.githubusercontent.com/{USER}/{REPO}/{BRANCH}/{PATH_IN_REPO}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b12c91c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2591,\n",
       " ['Lullaby of Birdland — George Shearing',\n",
       "  \"It's A Most Unusual Day — Jimmy McHugh and HYarold Adamson\",\n",
       "  'Jump Monk — Charles Mingus'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importamos el dataset de canciones con las progresiones funcionales\n",
    "df = pd.read_csv(URL)\n",
    "\n",
    "def tokenize_progression(prog: str):\n",
    "    \"\"\"\n",
    "    Tokeniza una progresión en acordes separados\n",
    "    \"\"\"\n",
    "    if pd.isna(prog):\n",
    "        return []\n",
    "    return [t for t in str(prog).strip().split() if t]\n",
    "\n",
    "def build_sequences_by_song(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Construye secuencias de acordes por canción a partir del DataFrame.\n",
    "    \"\"\"\n",
    "    if \"title\" in df.columns and \"composedby\" in df.columns:\n",
    "        song_ids = (df[\"title\"].astype(str) + \" — \" + df[\"composedby\"].astype(str)).tolist()\n",
    "    elif \"title\" in df.columns:\n",
    "        song_ids = df[\"title\"].astype(str).tolist()\n",
    "    else:\n",
    "        song_ids = [f\"song_{i}\" for i in range(len(df))]\n",
    "    seqs = {}\n",
    "    for sid, prog in zip(song_ids, df[\"funcional_prog\"].tolist()):\n",
    "        seqs[sid] = tokenize_progression(prog)\n",
    "    return seqs\n",
    "\n",
    "seqs = build_sequences_by_song(df)\n",
    "seqs = {k:v for k,v in seqs.items() if len(v) >= 3}\n",
    "len(seqs), list(seqs)[:3]  # tamaño y primeras claves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f268e8b",
   "metadata": {},
   "source": [
    "## 2) Partición train/val/test por canción\n",
    "Usamos 80/10/10 con barajado determinista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0ee5ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2072, 259, 260)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rng = np.random.default_rng(42)\n",
    "song_list = list(seqs.keys())\n",
    "rng.shuffle(song_list)\n",
    "\n",
    "n = len(song_list)\n",
    "n_train = int(n * 0.8)\n",
    "n_val   = int(n * 0.1)\n",
    "train_ids = song_list[:n_train]\n",
    "val_ids   = song_list[n_train:n_train+n_val]\n",
    "test_ids  = song_list[n_train+n_val:]\n",
    "\n",
    "train_seqs = [seqs[sid] for sid in train_ids]\n",
    "val_seqs   = [seqs[sid] for sid in val_ids]\n",
    "test_seqs  = [seqs[sid] for sid in test_ids]\n",
    "\n",
    "len(train_seqs), len(val_seqs), len(test_seqs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbcceb0",
   "metadata": {},
   "source": [
    "## 3) N-grama Kneser–Ney (trigrama)\n",
    "Implementamos Kneser–Ney interpolado con descuento absoluto `D=0.75`. Mapeamos a `<unk>` tokens con frecuencia ≤ 1 en train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e471a5f",
   "metadata": {},
   "source": [
    "El suavizado **Kneser–Ney (KN)** se diseñó para mejorar los modelos n-grama cuando hay **mucha dispersión** (en nuestro caso, muchos acordes posibles) y no siempre tenemos suficientes ejemplos de cada secuencia larga (como trigramas).\n",
    "\n",
    "Tres cuestiones clave:\n",
    "- **Descuento (D).**  \n",
    "  A cada conteo de n-grama observado le restamos una pequeña cantidad fija (por ejemplo 0.75).  \n",
    "  Así evitamos que los n-gramas muy frecuentes acaparen toda la probabilidad y liberamos un poco de “masa” para repartirla en otros casos.\n",
    "\n",
    "- **Redistribución hacia *continuations*.**  \n",
    "  La probabilidad “liberada” no se reparte de forma uniforme, sino en función de cuántos **contextos distintos** ha tenido un acorde.  \n",
    "  Ejemplo: un acorde que aparece después de muchos acordes diferentes recibe más probabilidad que otro que solo aparece en un contexto concreto, aunque ambos tengan el mismo número de ocurrencias totales.  \n",
    "  Esto hace que KN capte la **diversidad de contextos** en los que aparece un acorde, no solo su frecuencia absoluta.\n",
    "\n",
    "- **Back-off (interpolación con modelos de menor orden).**  \n",
    "  Si no encontramos un trigrama (tres acordes seguidos), en lugar de dar probabilidad cero, “retrocedemos” al modelo de bigramas. Si tampoco hay datos suficientes, retrocedemos al modelo de unigramas.  \n",
    "  KN combina de forma ponderada (con un peso λ) la probabilidad descontada del n-grama con las probabilidades de los modelos de menor orden.\n",
    "\n",
    "**En resumen:** Kneser–Ney reparte la probabilidad de manera más inteligente:  \n",
    "quita un poco de probabilidad a los casos frecuentes y la asigna a los acordes que han demostrado poder aparecer en **más contextos diferentes**. Esto mejora mucho la calidad de las predicciones cuando el dataset es limitado.\n",
    "\n",
    "\n",
    "**Hiperparámetros.**\n",
    "- Orden \\(N=3\\) (trigrama).\n",
    "- Descuento (típico: 0.75).\n",
    "- Umbral `unk_threshold` para aplanar acordes raros a `<unk>`.\n",
    "\n",
    "**Entrenamiento.**\n",
    "1. Normaliza y tokeniza la progresión y añade `<s>` al inicio y `</s>` al final.\n",
    "2. Cuenta n-gramas (uni-, bi- y trigramas).\n",
    "3. Se registran los *continuations*, es decir, cuántos contextos distintos puede seguir cada acorde.\n",
    "4. A partir de esos conteos se calculan los **pesos de retroceso (λ)**, que indican cuánta probabilidad hay que “devolver” a modelos más generales (bigramas y unigramas) cuando falta evidencia.\n",
    "\n",
    "**Ventajas y límites.**\n",
    "- Muy robusto en datos escasos; explicable.\n",
    "- No modela dependencias largas > N.\n",
    "- Sensible al vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bab6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario: 85\n"
     ]
    }
   ],
   "source": [
    "class KNTrigramLM:\n",
    "    def __init__(self, discount: float = 0.75, unk_threshold: int = 1):\n",
    "        self.D = discount # Descuento aplicado a los n-gramas más frecuentes\n",
    "        self.unk_threshold = unk_threshold # umbral para mapear tokens raros a <unk>\n",
    "        self.vocab = set() # vocabulario final (incluye <unk>, <s>, </s>)\n",
    "        self.uni = Counter(); self.bi = Counter(); self.tri = Counter() # contadores de n-gramas\n",
    "        self.continuation_counts = Counter() # nº de contextos únicos por token \n",
    "        self.bigram_continuation_counts = Counter() # nº de contextos únicos por bigrama \n",
    "        self.context_totals = Counter() \n",
    "        self.trigram_context_totals = Counter()\n",
    "        self.fitted = False\n",
    "\n",
    "    @staticmethod\n",
    "    def add_bounds(seq):\n",
    "        \"\"\"Añade los tokens de inicio y fin de secuencia\"\"\"\n",
    "        return [\"<s>\", \"<s>\"] + seq + [\"</s>\"]\n",
    "\n",
    "    def fit(self, sequences):\n",
    "        \"\"\"\n",
    "        Ajusta el modelo a las secuencias de entrenamiento.\n",
    "        - Construye el vocabulario (con <unk> para acordes raros).\n",
    "        - Añade tokens de inicio/fin (<s>, </s>).\n",
    "        - Cuenta unigramas, bigramas y trigramas.\n",
    "        - Calcula las estadísticas de continuación necesarias para Kneser–Ney.\n",
    "        \"\"\"\n",
    "        # 1) Construcción del vocabulario\n",
    "        token_counts = Counter(t for seq in sequences for t in seq)\n",
    "        vocab = set([t for t,c in token_counts.items() if c > self.unk_threshold])\n",
    "        vocab.update({\"<s>\", \"</s>\", \"<unk>\"}) # añadimos tokens especiales\n",
    "        self.vocab = vocab\n",
    "\n",
    "        def map_unk(seq):\n",
    "            \"\"\"Mapea tokens raros a <unk> \"\"\"\n",
    "            return [t if t in vocab else \"<unk>\" for t in seq]\n",
    "\n",
    "        uni, bi, tri = Counter(), Counter(), Counter()\n",
    "        left_contexts_for_w = defaultdict(set)\n",
    "        left_contexts_for_bigram = defaultdict(set)\n",
    "\n",
    "        #2) Recorrer secuencias, añadir bounds, mapear <unk> y contar n-gramas\n",
    "        for seq in sequences:\n",
    "            seq2 = self.add_bounds(map_unk(seq))\n",
    "            for i in range(len(seq2)):\n",
    "                w = seq2[i]; uni[w] += 1\n",
    "                if i >= 1:\n",
    "                    w1 = seq2[i-1]; bi[(w1, w)] += 1; left_contexts_for_w[w].add(w1)\n",
    "                if i >= 2:\n",
    "                    w2, w1 = seq2[i-2], seq2[i-1]\n",
    "                    tri[(w2, w1, w)] += 1; left_contexts_for_bigram[(w1, w)].add(w2)\n",
    "\n",
    "        # 3) Guardar conteos en el estado del modelo\n",
    "        self.uni, self.bi, self.tri = uni, bi, tri\n",
    "        for (w1, w), c in bi.items():\n",
    "            self.context_totals[w1] += c\n",
    "        for (w2, w1, w), c in tri.items():\n",
    "            self.trigram_context_totals[(w2, w1)] += c\n",
    "\n",
    "        # 4) Calcular los conteos de continuación\n",
    "        self.continuation_counts = Counter({w: len(ctxs) for w, ctxs in left_contexts_for_w.items()})\n",
    "        self.bigram_continuation_counts = Counter({(w1, w): len(ctxs) for (w1, w), ctxs in left_contexts_for_bigram.items()})\n",
    "        self.total_unique_bigrams = sum(1 for _ in self.bi.keys())\n",
    "        self.fitted = True\n",
    "\n",
    "    def prob_unigram(self, w):\n",
    "        \"\"\"Calcula la probabilidad de continuacion a nivel de unigram\"\"\"\n",
    "        cc = self.continuation_counts.get(w, 0)\n",
    "        if self.total_unique_bigrams == 0:\n",
    "            return 1.0 / max(1, len(self.vocab))\n",
    "        return cc / self.total_unique_bigrams\n",
    "\n",
    "    def prob_bigram(self, w_prev, w):\n",
    "        \"\"\"Calcula la probabilidad de continuacion a nivel de bigram.\n",
    "        Incluye backoff a unigram si la probabilidad de bigram es 0.\"\"\"\n",
    "        c_wprev = self.context_totals.get(w_prev, 0)\n",
    "        c_wprev_w = self.bi.get((w_prev, w), 0)\n",
    "        if c_wprev > 0:\n",
    "            lambda_wprev = (self.D * len([u for u in self.vocab if self.bi.get((w_prev, u), 0) > 0])) / c_wprev\n",
    "        else:\n",
    "            lambda_wprev = 1.0\n",
    "        p_cont = self.prob_unigram(w)\n",
    "        base = max(c_wprev_w - self.D, 0) / c_wprev if c_wprev > 0 else 0.0\n",
    "        return base + lambda_wprev * p_cont\n",
    "\n",
    "    def prob_trigram(self, w_prev2, w_prev, w):\n",
    "        \"\"\"Calcula la probabilidad de continuacion a nivel de trigram.\n",
    "        Incluye backoff a bigram si la probabilidad de trigram es 0.\"\"\"\n",
    "        c_ctx = self.trigram_context_totals.get((w_prev2, w_prev), 0)\n",
    "        c_trigram = self.tri.get((w_prev2, w_prev, w), 0)\n",
    "        if c_ctx > 0:\n",
    "            num_continuations = len([u for u in self.vocab if self.tri.get((w_prev2, w_prev, u), 0) > 0])\n",
    "            lambda_ctx = (self.D * num_continuations) / c_ctx\n",
    "        else:\n",
    "            lambda_ctx = 1.0\n",
    "        base = max(c_trigram - self.D, 0) / c_ctx if c_ctx > 0 else 0.0\n",
    "        return base + lambda_ctx * self.prob_bigram(w_prev, w)\n",
    "\n",
    "    def predict_ranking(self, history):\n",
    "        \"\"\"Dado un contexto (historia), devuelve una lista ordenada de\n",
    "        tuplas (token, probabilidad) para cada token en el vocabulario.\"\"\"\n",
    "        hist = [\"<s>\", \"<s>\"] + [t if t in self.vocab else \"<unk>\" for t in history]\n",
    "        w_prev2, w_prev = hist[-2], hist[-1]\n",
    "        candidates = [w for w in self.vocab if w not in {\"<s>\"}]\n",
    "        scores = []\n",
    "        for w in candidates:\n",
    "            p = self.prob_trigram(w_prev2, w_prev, w)\n",
    "            scores.append((w, p))\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return scores\n",
    "\n",
    "kn = KNTrigramLM(discount=0.75, unk_threshold=1)\n",
    "kn.fit(train_seqs)\n",
    "print(f\"Tamaño del vocabulario: {len(kn.vocab)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbc60d1",
   "metadata": {},
   "source": [
    "## 4) HMM (Hidden Markov Model) utilizando funciones (T/S/D)\n",
    "Derivamos el estado oculto de cada token (`T`, `S`, `D`) a partir del **grado** del romano (e.g., `I, III, VI → T`; `ii, IV → S`; `V, vii → D`). Tratamos `♭II, ♭VI, ♭VII` como **S** y `♭III` como **T**. Estimamos **transiciones** y **emisiones** con suavizado add-k."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac521d40",
   "metadata": {},
   "source": [
    "Modelamos la progresión como una cadena de **estados ocultos** de función armónica:\n",
    "**Tónica (T)**, **Subdominante (S)** y **Dominante (D)**. \n",
    "\n",
    "**Cómo mapeamos un acorde a T/S/D (según el código):**\n",
    "- Extraemos la base romana del token (antes de barras “/”) y detectamos bemol inicial (`b` o `♭`).  \n",
    "- Mapeo por grados (mayormente diatónico):\n",
    "  - **T (tónica)**: I, iii, vi y ♭III  \n",
    "  - **S (subdominante)**: ii, IV y ♭II, ♭VI, ♭VII  \n",
    "  - **D (dominante)**: V, vii  \n",
    "  - En caso no cubierto, cae en **T** por defecto.\n",
    "- Notas: `#`/`♯` no alteran la función en esta versión (solo distingimos bemol inicial). Dominantes secundarios y sustitutos se pierden en T/S/D.\n",
    "\n",
    "**Entrenamiento:**\n",
    "1. Para cada progresión, convertimos cada acorde a su función **T/S/D** con `function_from_token`.\n",
    "2. Contamos:\n",
    "   - **Iniciales**: frecuencia de cada estado inicial → π.  \n",
    "   - **Transiciones** entre estados consecutivos → matriz **A**.  \n",
    "   - **Emisiones** estado→acorde → matriz **B**.\n",
    "3. Aplicamos **suavizado add-k** (`k=0.5` en nuestro experimento) a π, A y B para evitar ceros.  \n",
    "   En **B** reservamos masa para `\"<unk>\"` (acordes fuera del vocabulario de *train*).\n",
    "\n",
    "**Inferencia:**\n",
    "- Si no hay historial: usamos **π** como distribución de estado.  \n",
    "- Si hay historial: tomamos **el último acorde**, lo mapeamos a **T/S/D** y usamos **A[s_last]** como distribución del siguiente estado.  \n",
    "- La probabilidad del **siguiente acorde** mezcla emisiones:  \n",
    "  \\( P(x_{t+1}) = \\sum_{z \\in \\{T,S,D\\}} P(x_{t+1}\\mid z)\\,P(z\\mid \\text{historia}) \\).  \n",
    "  Se rankean los acordes del vocabulario (se excluye `\"<unk>\"` de la salida final).\n",
    "\n",
    "**¿Por qué T/S/D?**\n",
    "- **Simplicidad y explicabilidad**: permite ver la lógica funcional (T→S→D) de forma compacta.  \n",
    "- **Robustez con datos limitados**: menos parámetros que un HMM con muchos estados.  \n",
    "  \n",
    "**Limitaciones conocidas**\n",
    "- **Pérdida de granularidad**: muchas funciones distintas (p. ej., V/ii, sustitutos por tritono, variantes maj7/m7) se colapsan en T/S/D → recomendaciones menos específicas.  \n",
    "- **Alteraciones complejas**: no afectan la función en esta versión.  \n",
    "- Con más tiempo, **ampliar estados** (p. ej., notación funcional explícita) podría **mejorar la competitividad** del HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9738ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'S', 'D'],\n",
       " [('T',\n",
       "   {'T': 0.44894101307819917,\n",
       "    'S': 0.3055823463600941,\n",
       "    'D': 0.24547664056170673})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ROMAN_MAP = {\"I\":1,\"II\":2,\"III\":3,\"IV\":4,\"V\":5,\"VI\":6,\"VII\":7}\n",
    "\n",
    "import re\n",
    "def extract_roman_base(token: str):\n",
    "    \"\"\"Extrae grado del acorde (solo valores I..VII sin alteraciones)\"\"\"\n",
    "    t = token.split('/')[0]\n",
    "    m = re.match(r'^[b♭#♯]*([ivIV]+)', t)\n",
    "    is_flat = bool(re.match(r'^[b♭]', t))\n",
    "    if not m: return (\"\", is_flat)\n",
    "    return (m.group(1).upper(), is_flat)\n",
    "\n",
    "def degree_from_roman(r: str) -> int:\n",
    "    \"\"\"Convierte un numeral romano (I..VII) en un entero (1..7).\n",
    "    Devuelve 0 si no es válido.\"\"\"\n",
    "    return ROMAN_MAP.get(r, 0)\n",
    "\n",
    "def function_from_token(token: str) -> str:\n",
    "    \"Heurística para mapear un acorde a su función T/S/D\"\n",
    "    base, is_flat = extract_roman_base(token)\n",
    "    deg = degree_from_roman(base)\n",
    "    if is_flat and deg in {2,6,7}:  # bII, bVI, bVII\n",
    "        return \"S\"\n",
    "    if is_flat and deg == 3:        # bIII\n",
    "        return \"T\"\n",
    "    if deg in {1,3,6}:\n",
    "        return \"T\"\n",
    "    if deg in {2,4}:\n",
    "        return \"S\"\n",
    "    if deg in {5,7}:\n",
    "        return \"D\"\n",
    "    return \"T\"\n",
    "\n",
    "STATES = [\"T\",\"S\",\"D\"]\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "class SupervisedHMM_TSD:\n",
    "    \"\"\"HMM supervisado con estados T/S/D.\"\"\"\n",
    "    def __init__(self, add_k: float = 1.0):\n",
    "        self.add_k = add_k\n",
    "        self.states = STATES\n",
    "        self.A = None; self.B = None; self.pi = None # parámetros del modelo\n",
    "        self.vocab = set() # acordes observados en el entrenamiento\n",
    "\n",
    "    def fit(self, train_sequences):\n",
    "        \"\"\"\n",
    "        Ajusta el HMM por conteo supervisado:\n",
    "        1) Mapea cada acorde de la secuencia a T/S/D (function_from_token).\n",
    "        2) Cuenta iniciales (π), transiciones (A) y emisiones (B).\n",
    "        3) Aplica suavizado add-k y normaliza.\n",
    "        \"\"\"\n",
    "        # Vocabulario de acordes observados (sin <unk> aquí; se añade en B)\n",
    "        tok_counts = Counter(t for seq in train_sequences for t in seq)\n",
    "        self.vocab = set(tok_counts.keys())\n",
    "\n",
    "        # Estructuras de conteo\n",
    "        trans = {s: Counter() for s in self.states} # A: conteos de transiciones estado→estado\n",
    "        emit  = {s: Counter() for s in self.states} # B: conteos de emisiones estado→token\n",
    "        pi_counts = Counter()                       # π: conteos de estados iniciales\n",
    "\n",
    "        # Recorrer secuencias y contar\n",
    "        for seq in train_sequences:\n",
    "            if not seq: continue\n",
    "            states_seq = [function_from_token(t) for t in seq]  # mapeo acorde→función\n",
    "            pi_counts[states_seq[0]] += 1                       # primer estado de la secuencia\n",
    "            for t, s in zip(seq, states_seq):                   # conteo de emisiones\n",
    "                emit[s][t] += 1\n",
    "            for s1, s2 in zip(states_seq[:-1], states_seq[1:]): # conteo de transiciones\n",
    "                trans[s1][s2] += 1\n",
    "\n",
    "        # Iniciales π con add-k\n",
    "        total_pi = sum(pi_counts.values()) + self.add_k * len(self.states)\n",
    "        self.pi = {s: (pi_counts[s] + self.add_k) / total_pi for s in self.states}\n",
    "\n",
    "        # Transiciones A con add-k\n",
    "        self.A = {}\n",
    "        for s in self.states:\n",
    "            total = sum(trans[s].values()) + self.add_k * len(self.states)\n",
    "            self.A[s] = {s2: (trans[s][s2] + self.add_k) / total for s2 in self.states}\n",
    "\n",
    "        # Emisiones B con add-k\n",
    "        self.B = {}\n",
    "        V = len(self.vocab)\n",
    "        for s in self.states:\n",
    "            total = sum(emit[s].values()) + self.add_k * (V + 1)\n",
    "            self.B[s] = defaultdict(float)\n",
    "            for w in self.vocab:\n",
    "                self.B[s][w] = (emit[s][w] + self.add_k) / total\n",
    "            self.B[s][\"<unk>\"] = self.add_k / total\n",
    "\n",
    "    def predict_ranking(self, history):\n",
    "        \"\"\"\n",
    "        Ranking de prob. para el siguiente acorde:\n",
    "        - Si no hay historial → usa π como distribución de estado.\n",
    "        - Si hay historial → mapea el último acorde a T/S/D y usa A[s_last] como dist. del estado siguiente.\n",
    "        - Mezcla de emisiones: P(x) = sum_s P(x|s) * P(s|historia).\n",
    "        - Devuelve lista (acorde, prob) ordenada desc. (sin \"<unk>\" en salida).\n",
    "        \"\"\"\n",
    "        if len(history) == 0:\n",
    "            state_probs = self.pi\n",
    "        else:\n",
    "            s_last = function_from_token(history[-1])\n",
    "            state_probs = self.A[s_last]\n",
    "\n",
    "        scores = {}\n",
    "        for w in list(self.vocab) + [\"<unk>\"]:\n",
    "            p = 0.0\n",
    "            for s_next in self.states:\n",
    "                p += state_probs[s_next] * self.B[s_next].get(w, self.B[s_next][\"<unk>\"])\n",
    "            scores[w] = p\n",
    "        scores.pop(\"<unk>\", None)\n",
    "        items = list(scores.items())\n",
    "        items.sort(key=lambda x: x[1], reverse=True)\n",
    "        return items\n",
    "\n",
    "hmm = SupervisedHMM_TSD(add_k=0.5)\n",
    "hmm.fit(train_seqs)\n",
    "STATES, list(hmm.A.items())[:1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984157df",
   "metadata": {},
   "source": [
    "## 5) Evaluación (Top-k, MRR)\n",
    "Medimos la calidad de **predicción del siguiente token** en test. Reportamos **Top@1/3/5** y **MRR**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7a4ef27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>positions</th>\n",
       "      <th>MRR</th>\n",
       "      <th>Top@1</th>\n",
       "      <th>Top@3</th>\n",
       "      <th>Top@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kneser–Ney (trigrama)</td>\n",
       "      <td>12779</td>\n",
       "      <td>0.56052</td>\n",
       "      <td>0.415212</td>\n",
       "      <td>0.651616</td>\n",
       "      <td>0.737538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HMM funciones (T/S/D)</td>\n",
       "      <td>12779</td>\n",
       "      <td>0.34576</td>\n",
       "      <td>0.191564</td>\n",
       "      <td>0.418890</td>\n",
       "      <td>0.512481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  positions      MRR     Top@1     Top@3     Top@5\n",
       "0  Kneser–Ney (trigrama)      12779  0.56052  0.415212  0.651616  0.737538\n",
       "1  HMM funciones (T/S/D)      12779  0.34576  0.191564  0.418890  0.512481"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def evaluate_next_token_ranking(model, test_sequences, topk_list=(1,3,5)):\n",
    "    total_positions = 0\n",
    "    topk_hits = {k: 0 for k in topk_list}\n",
    "    mrr_sum = 0.0\n",
    "    for seq in test_sequences:\n",
    "        for i in range(len(seq)-1):\n",
    "            history = seq[:i+1]\n",
    "            gold = seq[i+1]\n",
    "            ranking = model.predict_ranking(history)\n",
    "            ranks = {w: r+1 for r, (w, _) in enumerate(ranking)}\n",
    "            rank_gold = ranks.get(gold, None)\n",
    "            total_positions += 1\n",
    "            if rank_gold is not None:\n",
    "                for k in topk_list:\n",
    "                    if rank_gold <= k:\n",
    "                        topk_hits[k] += 1\n",
    "                mrr_sum += 1.0 / rank_gold\n",
    "    results = {\n",
    "        \"positions\": total_positions,\n",
    "        \"MRR\": mrr_sum / total_positions if total_positions > 0 else 0.0,\n",
    "    }\n",
    "    for k in topk_list:\n",
    "        results[f\"Top@{k}\"] = topk_hits[k] / total_positions if total_positions > 0 else 0.0\n",
    "    return results\n",
    "\n",
    "class KNWrapperForEval:\n",
    "    def __init__(self, kn_model):\n",
    "        self.kn = kn_model\n",
    "    def predict_ranking(self, history):\n",
    "        hist = [t if t in self.kn.vocab else \"<unk>\" for t in history]\n",
    "        return self.kn.predict_ranking(hist)\n",
    "\n",
    "kn_res = evaluate_next_token_ranking(KNWrapperForEval(kn), test_seqs, topk_list=(1,3,5))\n",
    "hmm_res = evaluate_next_token_ranking(hmm, test_seqs, topk_list=(1,3,5))\n",
    "pd.DataFrame([{\"Model\":\"Kneser–Ney (trigrama)\", **kn_res},\n",
    "              {\"Model\":\"HMM funciones (T/S/D)\", **hmm_res}])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ac42c7",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "El modelo **Kneser–Ney (trigrama)** supera claramente al **HMM (T–S–D)** en todas las métricas de evaluación.\n",
    "\n",
    "**Interpretación de las métricas:**\n",
    "- **positions**: 12 779 puntos de predicción evaluados (idénticos para ambos modelos).\n",
    "- **Top@k**: proporción de veces que el acorde real aparece entre las *k* mejores sugerencias del modelo.\n",
    "  - **Top@1** (acierto exacto): 41.52% (KN) vs 19.15% (HMM).  \n",
    "  - **Top@3**: 65.16% (KN) vs 41.89% (HMM).  \n",
    "  - **Top@5**: 73.75% (KN) vs 51.24% (HMM).\n",
    "- **MRR (Mean Reciprocal Rank)**: mide la posición media en la que aparece el acorde correcto dentro del ranking. Cuanto más cerca del primer lugar, mayor es el valor.  \n",
    "  0.5605 (KN) vs 0.3457 (HMM) confirma el mejor ranking global del trigrama KN.\n",
    "\n",
    "**Interpretación.**\n",
    "- El **KN trigrama** es capaz de capturar **dependencias locales específicas entre acordes** y asignar probabilidades más precisas a las continuaciones típicas.  \n",
    "- El **HMM con solo 3 estados (T/S/D)** pierde granularidad: muchos acordes diferentes se colapsan en la misma categoría funcional, lo que reparte la probabilidad de manera más difusa y reduce la precisión de las predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db835d28",
   "metadata": {},
   "source": [
    "## 6) Roadmap (siguientes mejoras)\n",
    "- Añadir **KN 4-grama** y optimización de `D` vía validación.\n",
    "- Para HMM: probar **EM (Baum–Welch)** (peores resultados, vía descartada).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
