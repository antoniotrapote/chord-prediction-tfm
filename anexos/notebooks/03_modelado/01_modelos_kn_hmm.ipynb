{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1065c86",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/antoniotrapote/chord-prediction-tfm/blob/main/anexos/notebooks/03_modelado/01_modelos_kn_hmm.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-black?logo=github)](https://github.com/antoniotrapote/chord-prediction-tfm/blob/main/anexos/notebooks/03_modelado/01_modelos_kn_hmm.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a8d5a",
   "metadata": {},
   "source": [
    "# Modelos tradicionales: Kneser–Ney y HMM (T/S/D)\n",
    "Este notebook implementa dos baselines sobre **progresiones funcionales** a partir de `songdb_funcional_v4.csv`:\n",
    "\n",
    "- **Modelo n-grama con Kneser–Ney (trigrama)** sobre tokens funcionales (e.g., `I`, `ii`, `V7`, `bVII7`), con `<unk>` para raros y límites `<s>`, `</s>`.\n",
    "- **HMM de funciones (T/S/D)** con estados ocultos **Tónica (T)**, **Subdominante (S)** y **Dominante (D)** derivados heurísticamente de la etiqueta funcional; emisiones son **tokens funcionales**.\n",
    "\n",
    "Se evalúa **predicción del siguiente acorde** con **Top-k** y **MRR**, con partición **train/val/test por canción**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8366f8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53882b3",
   "metadata": {},
   "source": [
    "## 1) Cargamos el dataset\n",
    "Leemos el CSV y preparamos las secuencias de tokens por canción. Usamos `title` + `composedby` como ID de canción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d5df93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración para acceder al dataset `songdb_funcional_v4.csv` en GitHub\n",
    "USER = \"antoniotrapote\"\n",
    "REPO = \"chord-prediction-tfm\"\n",
    "BRANCH = \"main\"\n",
    "PATH_IN_REPO = \"anexos/data/songdb_funcional_v4.csv\"\n",
    "URL = f\"https://raw.githubusercontent.com/{USER}/{REPO}/{BRANCH}/{PATH_IN_REPO}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b12c91c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2591,\n",
       " ['Lullaby of Birdland — George Shearing',\n",
       "  \"It's A Most Unusual Day — Jimmy McHugh and HYarold Adamson\",\n",
       "  'Jump Monk — Charles Mingus'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importamos el dataset de canciones con las progresiones funcionales\n",
    "df = pd.read_csv(URL)\n",
    "\n",
    "def tokenize_progression(prog: str):\n",
    "    \"\"\"\n",
    "    Tokeniza una progresión en acordes separados\n",
    "    \"\"\"\n",
    "    if pd.isna(prog):\n",
    "        return []\n",
    "    return [t for t in str(prog).strip().split() if t]\n",
    "\n",
    "def build_sequences_by_song(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Construye secuencias de acordes por canción a partir del DataFrame.\n",
    "    \"\"\"\n",
    "    if \"title\" in df.columns and \"composedby\" in df.columns:\n",
    "        song_ids = (df[\"title\"].astype(str) + \" — \" + df[\"composedby\"].astype(str)).tolist()\n",
    "    elif \"title\" in df.columns:\n",
    "        song_ids = df[\"title\"].astype(str).tolist()\n",
    "    else:\n",
    "        song_ids = [f\"song_{i}\" for i in range(len(df))]\n",
    "    seqs = {}\n",
    "    for sid, prog in zip(song_ids, df[\"funcional_prog\"].tolist()):\n",
    "        seqs[sid] = tokenize_progression(prog)\n",
    "    return seqs\n",
    "\n",
    "seqs = build_sequences_by_song(df)\n",
    "seqs = {k:v for k,v in seqs.items() if len(v) >= 3}\n",
    "len(seqs), list(seqs)[:3]  # tamaño y primeras claves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f268e8b",
   "metadata": {},
   "source": [
    "## 2) Partición train/val/test por canción\n",
    "Usamos 80/10/10 con barajado determinista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0ee5ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2072, 259, 260)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rng = np.random.default_rng(42)\n",
    "song_list = list(seqs.keys())\n",
    "rng.shuffle(song_list)\n",
    "\n",
    "n = len(song_list)\n",
    "n_train = int(n * 0.8)\n",
    "n_val   = int(n * 0.1)\n",
    "train_ids = song_list[:n_train]\n",
    "val_ids   = song_list[n_train:n_train+n_val]\n",
    "test_ids  = song_list[n_train+n_val:]\n",
    "\n",
    "train_seqs = [seqs[sid] for sid in train_ids]\n",
    "val_seqs   = [seqs[sid] for sid in val_ids]\n",
    "test_seqs  = [seqs[sid] for sid in test_ids]\n",
    "\n",
    "len(train_seqs), len(val_seqs), len(test_seqs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbcceb0",
   "metadata": {},
   "source": [
    "## 3) N-grama Kneser–Ney (trigrama)\n",
    "Implementamos Kneser–Ney interpolado con descuento absoluto `D=0.75`. Mapeamos a `<unk>` tokens con frecuencia ≤ 1 en train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58bab6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class KNTrigramLM:\n",
    "    def __init__(self, discount: float = 0.75, unk_threshold: int = 1):\n",
    "        self.D = discount\n",
    "        self.unk_threshold = unk_threshold\n",
    "        self.vocab = set()\n",
    "        self.uni = Counter(); self.bi = Counter(); self.tri = Counter()\n",
    "        self.continuation_counts = Counter()\n",
    "        self.bigram_continuation_counts = Counter()\n",
    "        self.context_totals = Counter()\n",
    "        self.trigram_context_totals = Counter()\n",
    "        self.fitted = False\n",
    "\n",
    "    @staticmethod\n",
    "    def add_bounds(seq):\n",
    "        return [\"<s>\", \"<s>\"] + seq + [\"</s>\"]\n",
    "\n",
    "    def fit(self, sequences):\n",
    "        token_counts = Counter(t for seq in sequences for t in seq)\n",
    "        vocab = set([t for t,c in token_counts.items() if c > self.unk_threshold])\n",
    "        vocab.update({\"<s>\", \"</s>\", \"<unk>\"})\n",
    "        self.vocab = vocab\n",
    "\n",
    "        def map_unk(seq):\n",
    "            return [t if t in vocab else \"<unk>\" for t in seq]\n",
    "\n",
    "        uni, bi, tri = Counter(), Counter(), Counter()\n",
    "        left_contexts_for_w = defaultdict(set)\n",
    "        left_contexts_for_bigram = defaultdict(set)\n",
    "\n",
    "        for seq in sequences:\n",
    "            seq2 = self.add_bounds(map_unk(seq))\n",
    "            for i in range(len(seq2)):\n",
    "                w = seq2[i]; uni[w] += 1\n",
    "                if i >= 1:\n",
    "                    w1 = seq2[i-1]; bi[(w1, w)] += 1; left_contexts_for_w[w].add(w1)\n",
    "                if i >= 2:\n",
    "                    w2, w1 = seq2[i-2], seq2[i-1]\n",
    "                    tri[(w2, w1, w)] += 1; left_contexts_for_bigram[(w1, w)].add(w2)\n",
    "\n",
    "        self.uni, self.bi, self.tri = uni, bi, tri\n",
    "        for (w1, w), c in bi.items():\n",
    "            self.context_totals[w1] += c\n",
    "        for (w2, w1, w), c in tri.items():\n",
    "            self.trigram_context_totals[(w2, w1)] += c\n",
    "\n",
    "        self.continuation_counts = Counter({w: len(ctxs) for w, ctxs in left_contexts_for_w.items()})\n",
    "        self.bigram_continuation_counts = Counter({(w1, w): len(ctxs) for (w1, w), ctxs in left_contexts_for_bigram.items()})\n",
    "        self.total_unique_bigrams = sum(1 for _ in self.bi.keys())\n",
    "        self.fitted = True\n",
    "\n",
    "    def prob_unigram(self, w):\n",
    "        cc = self.continuation_counts.get(w, 0)\n",
    "        if self.total_unique_bigrams == 0:\n",
    "            return 1.0 / max(1, len(self.vocab))\n",
    "        return cc / self.total_unique_bigrams\n",
    "\n",
    "    def prob_bigram(self, w_prev, w):\n",
    "        c_wprev = self.context_totals.get(w_prev, 0)\n",
    "        c_wprev_w = self.bi.get((w_prev, w), 0)\n",
    "        if c_wprev > 0:\n",
    "            lambda_wprev = (self.D * len([u for u in self.vocab if self.bi.get((w_prev, u), 0) > 0])) / c_wprev\n",
    "        else:\n",
    "            lambda_wprev = 1.0\n",
    "        p_cont = self.prob_unigram(w)\n",
    "        base = max(c_wprev_w - self.D, 0) / c_wprev if c_wprev > 0 else 0.0\n",
    "        return base + lambda_wprev * p_cont\n",
    "\n",
    "    def prob_trigram(self, w_prev2, w_prev, w):\n",
    "        c_ctx = self.trigram_context_totals.get((w_prev2, w_prev), 0)\n",
    "        c_trigram = self.tri.get((w_prev2, w_prev, w), 0)\n",
    "        if c_ctx > 0:\n",
    "            num_continuations = len([u for u in self.vocab if self.tri.get((w_prev2, w_prev, u), 0) > 0])\n",
    "            lambda_ctx = (self.D * num_continuations) / c_ctx\n",
    "        else:\n",
    "            lambda_ctx = 1.0\n",
    "        base = max(c_trigram - self.D, 0) / c_ctx if c_ctx > 0 else 0.0\n",
    "        return base + lambda_ctx * self.prob_bigram(w_prev, w)\n",
    "\n",
    "    def predict_ranking(self, history):\n",
    "        hist = [\"<s>\", \"<s>\"] + [t if t in self.vocab else \"<unk>\" for t in history]\n",
    "        w_prev2, w_prev = hist[-2], hist[-1]\n",
    "        candidates = [w for w in self.vocab if w not in {\"<s>\"}]\n",
    "        scores = []\n",
    "        for w in candidates:\n",
    "            p = self.prob_trigram(w_prev2, w_prev, w)\n",
    "            scores.append((w, p))\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return scores\n",
    "\n",
    "kn = KNTrigramLM(discount=0.75, unk_threshold=1)\n",
    "kn.fit(train_seqs)\n",
    "len(kn.vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbc60d1",
   "metadata": {},
   "source": [
    "## 4) HMM (Hidden Markov model) utilizando funciones (T/S/D)\n",
    "Derivamos el estado oculto de cada token (`T`, `S`, `D`) a partir del **grado** del romano (e.g., `I, III, VI → T`; `ii, IV → S`; `V, vii → D`). Tratamos `♭II, ♭VI, ♭VII` como **S** y `♭III` como **T**. Estimamos **transiciones** y **emisiones** con suavizado add-k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9738ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'S', 'D'],\n",
       " [('T',\n",
       "   {'T': 0.44894101307819917,\n",
       "    'S': 0.3055823463600941,\n",
       "    'D': 0.24547664056170673})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ROMAN_MAP = {\"I\":1,\"II\":2,\"III\":3,\"IV\":4,\"V\":5,\"VI\":6,\"VII\":7}\n",
    "\n",
    "import re\n",
    "def extract_roman_base(token: str):\n",
    "    t = token.split('/')[0]\n",
    "    m = re.match(r'^[b♭#♯]*([ivIV]+)', t)\n",
    "    is_flat = bool(re.match(r'^[b♭]', t))\n",
    "    if not m: return (\"\", is_flat)\n",
    "    return (m.group(1).upper(), is_flat)\n",
    "\n",
    "def degree_from_roman(r: str) -> int:\n",
    "    return ROMAN_MAP.get(r, 0)\n",
    "\n",
    "def function_from_token(token: str) -> str:\n",
    "    base, is_flat = extract_roman_base(token)\n",
    "    deg = degree_from_roman(base)\n",
    "    if is_flat and deg in {2,6,7}:  # bII, bVI, bVII\n",
    "        return \"S\"\n",
    "    if is_flat and deg == 3:        # bIII\n",
    "        return \"T\"\n",
    "    if deg in {1,3,6}:\n",
    "        return \"T\"\n",
    "    if deg in {2,4}:\n",
    "        return \"S\"\n",
    "    if deg in {5,7}:\n",
    "        return \"D\"\n",
    "    return \"T\"\n",
    "\n",
    "STATES = [\"T\",\"S\",\"D\"]\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "class SupervisedHMM_TSD:\n",
    "    def __init__(self, add_k: float = 1.0):\n",
    "        self.add_k = add_k\n",
    "        self.states = STATES\n",
    "        self.A = None; self.B = None; self.pi = None\n",
    "        self.vocab = set()\n",
    "\n",
    "    def fit(self, train_sequences):\n",
    "        tok_counts = Counter(t for seq in train_sequences for t in seq)\n",
    "        self.vocab = set(tok_counts.keys())\n",
    "        trans = {s: Counter() for s in self.states}\n",
    "        emit  = {s: Counter() for s in self.states}\n",
    "        pi_counts = Counter()\n",
    "\n",
    "        for seq in train_sequences:\n",
    "            if not seq: continue\n",
    "            states_seq = [function_from_token(t) for t in seq]\n",
    "            pi_counts[states_seq[0]] += 1\n",
    "            for t, s in zip(seq, states_seq):\n",
    "                emit[s][t] += 1\n",
    "            for s1, s2 in zip(states_seq[:-1], states_seq[1:]):\n",
    "                trans[s1][s2] += 1\n",
    "\n",
    "        total_pi = sum(pi_counts.values()) + self.add_k * len(self.states)\n",
    "        self.pi = {s: (pi_counts[s] + self.add_k) / total_pi for s in self.states}\n",
    "\n",
    "        self.A = {}\n",
    "        for s in self.states:\n",
    "            total = sum(trans[s].values()) + self.add_k * len(self.states)\n",
    "            self.A[s] = {s2: (trans[s][s2] + self.add_k) / total for s2 in self.states}\n",
    "\n",
    "        self.B = {}\n",
    "        V = len(self.vocab)\n",
    "        for s in self.states:\n",
    "            total = sum(emit[s].values()) + self.add_k * (V + 1)\n",
    "            self.B[s] = defaultdict(float)\n",
    "            for w in self.vocab:\n",
    "                self.B[s][w] = (emit[s][w] + self.add_k) / total\n",
    "            self.B[s][\"<unk>\"] = self.add_k / total\n",
    "\n",
    "    def predict_ranking(self, history):\n",
    "        if len(history) == 0:\n",
    "            state_probs = self.pi\n",
    "        else:\n",
    "            s_last = function_from_token(history[-1])\n",
    "            state_probs = self.A[s_last]\n",
    "\n",
    "        scores = {}\n",
    "        for w in list(self.vocab) + [\"<unk>\"]:\n",
    "            p = 0.0\n",
    "            for s_next in self.states:\n",
    "                p += state_probs[s_next] * self.B[s_next].get(w, self.B[s_next][\"<unk>\"])\n",
    "            scores[w] = p\n",
    "        scores.pop(\"<unk>\", None)\n",
    "        items = list(scores.items())\n",
    "        items.sort(key=lambda x: x[1], reverse=True)\n",
    "        return items\n",
    "\n",
    "hmm = SupervisedHMM_TSD(add_k=0.5)\n",
    "hmm.fit(train_seqs)\n",
    "STATES, list(hmm.A.items())[:1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984157df",
   "metadata": {},
   "source": [
    "## 5) Evaluación (Top-k, MRR)\n",
    "Medimos la calidad de **predicción del siguiente token** en test. Reportamos **Top@1/3/5** y **MRR**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7a4ef27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>positions</th>\n",
       "      <th>MRR</th>\n",
       "      <th>Top@1</th>\n",
       "      <th>Top@3</th>\n",
       "      <th>Top@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kneser–Ney (trigrama)</td>\n",
       "      <td>12779</td>\n",
       "      <td>0.56052</td>\n",
       "      <td>0.415212</td>\n",
       "      <td>0.651616</td>\n",
       "      <td>0.737538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HMM funciones (T/S/D)</td>\n",
       "      <td>12779</td>\n",
       "      <td>0.34576</td>\n",
       "      <td>0.191564</td>\n",
       "      <td>0.418890</td>\n",
       "      <td>0.512481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  positions      MRR     Top@1     Top@3     Top@5\n",
       "0  Kneser–Ney (trigrama)      12779  0.56052  0.415212  0.651616  0.737538\n",
       "1  HMM funciones (T/S/D)      12779  0.34576  0.191564  0.418890  0.512481"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def evaluate_next_token_ranking(model, test_sequences, topk_list=(1,3,5)):\n",
    "    total_positions = 0\n",
    "    topk_hits = {k: 0 for k in topk_list}\n",
    "    mrr_sum = 0.0\n",
    "    for seq in test_sequences:\n",
    "        for i in range(len(seq)-1):\n",
    "            history = seq[:i+1]\n",
    "            gold = seq[i+1]\n",
    "            ranking = model.predict_ranking(history)\n",
    "            ranks = {w: r+1 for r, (w, _) in enumerate(ranking)}\n",
    "            rank_gold = ranks.get(gold, None)\n",
    "            total_positions += 1\n",
    "            if rank_gold is not None:\n",
    "                for k in topk_list:\n",
    "                    if rank_gold <= k:\n",
    "                        topk_hits[k] += 1\n",
    "                mrr_sum += 1.0 / rank_gold\n",
    "    results = {\n",
    "        \"positions\": total_positions,\n",
    "        \"MRR\": mrr_sum / total_positions if total_positions > 0 else 0.0,\n",
    "    }\n",
    "    for k in topk_list:\n",
    "        results[f\"Top@{k}\"] = topk_hits[k] / total_positions if total_positions > 0 else 0.0\n",
    "    return results\n",
    "\n",
    "class KNWrapperForEval:\n",
    "    def __init__(self, kn_model):\n",
    "        self.kn = kn_model\n",
    "    def predict_ranking(self, history):\n",
    "        hist = [t if t in self.kn.vocab else \"<unk>\" for t in history]\n",
    "        return self.kn.predict_ranking(hist)\n",
    "\n",
    "kn_res = evaluate_next_token_ranking(KNWrapperForEval(kn), test_seqs, topk_list=(1,3,5))\n",
    "hmm_res = evaluate_next_token_ranking(hmm, test_seqs, topk_list=(1,3,5))\n",
    "pd.DataFrame([{\"Model\":\"Kneser–Ney (trigrama)\", **kn_res},\n",
    "              {\"Model\":\"HMM funciones (T/S/D)\", **hmm_res}])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ac42c7",
   "metadata": {},
   "source": [
    "Kneser–Ney (trigrama) supera claramente al HMM en todas las métricas.\n",
    "\n",
    "Interpretación de columnas:\n",
    "- positions: 12 779 puntos de predicción evaluados (mismas posiciones para ambos).\n",
    "- Top@k: proporción de veces que el acorde real aparece entre las k mejores sugerencias.\n",
    "  - Top@1: acierto exacto. 41.52% vs 19.15%.\n",
    "  - Top@3: el LM cubre 65.16% frente a 41.89%.\n",
    "  - Top@5: 73.75% vs 51.24%.\n",
    "- MRR: media de 1/rango (sensible a colocar el gold alto). 0.5605 vs 0.3457 confirma mejor ranking global del trigram KN.\n",
    "\n",
    "Conclusión: El modelo KN capta dependencias locales específicas entre acordes; el HMM (solo 3 estados T/S/D + emisiones) pierde granularidad y distribuye probabilidad más difusa, bajando precisión. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db835d28",
   "metadata": {},
   "source": [
    "## 6) Roadmap (siguientes mejoras)\n",
    "- Añadir **KN 4-grama** y optimización de `D` vía validación.\n",
    "- Para HMM: probar **EM (Baum–Welch)** (peores resultados, vía descartada).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
