{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b05592c6",
      "metadata": {
        "id": "b05592c6"
      },
      "source": [
        "**Nota**: Se recomienda ejecutar este notebook en Google Colab para asegurar la compatibilidad y evitar problemas de dependencias. El entrenamiento de los modelos requiere una cantidad significativa de memoria RAM y potencia de cómputo, que puede no estar disponible en todos los entornos locales.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/antoniotrapote/chord-prediction-tfm/blob/main/anexos/notebooks/03_modelado/04_modelo_lstm.ipynb)\n",
        "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-black?logo=github)](https://github.com/antoniotrapote/chord-prediction-tfm/blob/main/anexos/notebooks/03_modelado/04_modelo_lstm.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36e6ce9a",
      "metadata": {
        "id": "36e6ce9a"
      },
      "source": [
        "# Long Short-Term Memory (LSTM) model - PyTorch\n",
        "En este notebook hemos utilizado PyTorch para implementar y entrenar un modelo de red neuronal recurrente basado en Long Short-Term Memory (LSTM) para la predicción de acordes en secuencias musicales.\n",
        "\n",
        "**dataset**: `songdb_funcional_v4`\n",
        "\n",
        "**Contenido del notebook**:\n",
        "1. Entorno (Colab) - comprobación de versiones\n",
        "2. Traer el CSV desde GitHub  \n",
        "3. Cargar el CSV + tokenizar.  \n",
        "4. Split train/val/test.  \n",
        "5. Vocabulario + codificación.\n",
        "6. Dataset + DataLoaders.\n",
        "7. Modelo LSTM.  \n",
        "8. Definimos el entrenamiento y las métricas (Top@k, MRR, PPL).  \n",
        "9. Entrenamos el modelo.  \n",
        "10. Evaluación en test.  \n",
        "11. Conclusiones.  \n",
        "12. Función para predicciones `predict_next(context, k=5)`.\n",
        "13. Inferencia incremental (quick test).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00436936",
      "metadata": {
        "id": "00436936"
      },
      "source": [
        "## 1) Entorno (Colab)\n",
        "El modelo fue originalmente entrenado en Google Colab, con las siguientes especificaciones:\n",
        ">Python: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]  \n",
        ">PyTorch: 2.8.0+cu126  \n",
        ">CUDA disponible: True  \n",
        ">GPU: Tesla T4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "C38G_ux7fg8i",
      "metadata": {
        "id": "C38G_ux7fg8i"
      },
      "outputs": [],
      "source": [
        "#@title Semillas y determinismo\n",
        "import random, os, numpy as np, torch\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "48d96011",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48d96011",
        "outputId": "ab741a5d-9e2a-4eb4-84db-05b0f30b6410"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
            "PyTorch: 2.8.0+cu126\n",
            "CUDA disponible: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "#@title Comprobar GPU/versions\n",
        "import sys, torch\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"⚠️ Activa GPU: Runtime ▶ Change runtime type ▶ GPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ddef432",
      "metadata": {
        "id": "9ddef432"
      },
      "source": [
        "## 2) Traer el CSV desde GitHub\n",
        "Descargamos el dataset procesado desde el repositorio de GitHub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9e7eb715",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e7eb715",
        "outputId": "60bd3bd5-207d-41cc-d608-537e7d1774fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset descargado en: /content/songdb_funcional_v4.csv\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "\n",
        "# Configuración para descargar el dataset desde GitHub\n",
        "USER = \"antoniotrapote\"\n",
        "REPO = \"chord-prediction-tfm\"\n",
        "BRANCH = \"main\"\n",
        "PATH_IN_REPO = \"anexos/data/songdb_funcional_v4.csv\"\n",
        "URL = f\"https://raw.githubusercontent.com/{USER}/{REPO}/{BRANCH}/{PATH_IN_REPO}\"\n",
        "\n",
        "# Ruta local donde guardar el archivo\n",
        "data_path = \"/content/songdb_funcional_v4.csv\"\n",
        "\n",
        "# Descargar el archivo CSV desde GitHub\n",
        "urllib.request.urlretrieve(URL, data_path)\n",
        "print(f\"Dataset descargado en: {data_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce4a6a9c",
      "metadata": {
        "id": "ce4a6a9c"
      },
      "source": [
        "## 3) Cargar CSV y tokenizar\n",
        "Cargamos el archivo CSV en un DataFrame de pandas y transformamos las secuencias de acordes en listas de tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "23512052",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "23512052",
        "outputId": "eb836fe4-b99e-4b56-cc34-a6f8e2ad760e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filas totales: 2613\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"print(f\\\"Filas tras filtro min_seq_len >= {min_seq_len}:\\\", len(df))\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"funcional_prog\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"vi #iv\\u00f8 V/III V/VI vi IV ii V7 iii vi ii V7 I IV7 vii\\u00f8 V/VI vi #iv\\u00f8 V/III V/VI vi IV ii V7 iii vi ii V7 I V7 I V/II ii ii V7 I V/II ii V7 I V/VI vi #iv\\u00f8 V/III V/VI vi IV ii V7 iii vi ii V7 I V7 I III7\",\n          \"VII VII I vi ii V7 VII VII I vi ii V7 I IV #ivo I iii vi V/V V7 VII VII I vi ii V7 VII VII I vi ii V7 I IV #ivo I ii V7 I I vi bvi v V/IV IV IV vi bvi v V/IV IV IV vii bvii vi V/V V V ii V7 ii V7 VII VII I vi ii V7 VII VII I vi ii V7 I IV #ivo I iii vi V/V V7 iii IV iii IV iii ii ii V7 I V7\",\n          \"i VI V/V V7 i VI V/V V7 i VI ii\\u00f8 V7 i VI ii\\u00f8 V7 i i Vsub/V V7 iv iv ii\\u00f8 bII7 iv bII v\\u00f8 V/IV iv bII v\\u00f8 V/IV iv bII v\\u00f8 V/IV iv Vsub/II ii\\u00f8 V7 i VI ii\\u00f8 V7 i VI ii\\u00f8 V7 i i Vsub/V V7 iv iv ii\\u00f8 bII7 bII bII\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-192f86b1-d863-4629-a98b-7edf9a653e4a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>funcional_prog</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>vi #ivø V/III V/VI vi IV ii V7 iii vi ii V7 I ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>VII VII I vi ii V7 VII VII I vi ii V7 I IV #iv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i VI V/V V7 i VI V/V V7 i VI iiø V7 i VI iiø V...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-192f86b1-d863-4629-a98b-7edf9a653e4a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-192f86b1-d863-4629-a98b-7edf9a653e4a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-192f86b1-d863-4629-a98b-7edf9a653e4a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-cbc80490-ed12-45e3-86c1-2e5e075da23d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cbc80490-ed12-45e3-86c1-2e5e075da23d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-cbc80490-ed12-45e3-86c1-2e5e075da23d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                      funcional_prog\n",
              "0  vi #ivø V/III V/VI vi IV ii V7 iii vi ii V7 I ...\n",
              "1  VII VII I vi ii V7 VII VII I vi ii V7 I IV #iv...\n",
              "2  i VI V/V V7 i VI V/V V7 i VI iiø V7 i VI iiø V..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filas tras filtro min_seq_len >= 8: 2612\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd, ast, re\n",
        "\n",
        "# Configuración de datos\n",
        "sequence_col = \"funcional_prog\"  # Columna de secuencia (\"chordprog\" para cifrado americano originales)\n",
        "min_seq_len = 8  # Ignora secuencias muy cortas\n",
        "\n",
        "df = pd.read_csv(data_path)\n",
        "assert sequence_col in df.columns, f\"Columna {sequence_col} no encontrada en el CSV.\"\n",
        "print(\"Filas totales:\", len(df))\n",
        "display(df[[sequence_col]].head(3))\n",
        "\n",
        "def parse_tokens_simple(s: str):\n",
        "  \"\"\"Si viene como lista en string, intenta parsear\"\"\"\n",
        "  if isinstance(s, str) and s.strip().startswith(\"[\") and s.strip().endswith(\"]\"):\n",
        "    try:\n",
        "      lst = ast.literal_eval(s)\n",
        "      if isinstance(lst, list):\n",
        "        return [str(t) for t in lst]\n",
        "    except Exception:\n",
        "      pass\n",
        "\n",
        "  # Normaliza separadores de compás y saltos de línea a espacios\n",
        "  s = str(s).replace(\"|\", \" \").replace(\"\\n\", \" \")\n",
        "  toks = [t for t in re.findall(r\"\\S+\", s) if t.strip()]\n",
        "  return toks\n",
        "\n",
        "# Tokenizar y filtrar secuencias muy cortas\n",
        "df[\"_tokens_\"] = df[sequence_col].apply(parse_tokens_simple)\n",
        "df = df[df[\"_tokens_\"].apply(len) >= min_seq_len].reset_index(drop=True)\n",
        "print(f\"Filas tras filtro min_seq_len >= {min_seq_len}:\", len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "395c628f",
      "metadata": {
        "id": "395c628f"
      },
      "source": [
        "## 4) Split train/val/test\n",
        "Dividimos el dataset en conjuntos de entrenamiento, validación y test para evaluar el modelo de manera adecuada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6bd427ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bd427ae",
        "outputId": "a68f9430-f148-4e2f-dd2e-7eb56a65e8a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 2089, Val: 261, Test: 262\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Parámetros de división del dataset\n",
        "val_size = 0.10     # 10% para validación\n",
        "test_size = 0.10    # 10% para test\n",
        "random_state = 42   # Semilla para reproducibilidad\n",
        "\n",
        "# Dividir en train/val/test\n",
        "train_df, tmp_df = train_test_split(df, test_size=val_size+test_size, random_state=random_state, shuffle=True)\n",
        "rel_test = test_size / (val_size + test_size) if (val_size + test_size) > 0 else 0.5\n",
        "val_df, test_df = train_test_split(tmp_df, test_size=rel_test, random_state=random_state, shuffle=True)\n",
        "\n",
        "# Extraer las secuencias tokenizadas\n",
        "train_seqs = train_df[\"_tokens_\"].tolist()\n",
        "val_seqs   = val_df[\"_tokens_\"].tolist()\n",
        "test_seqs  = test_df[\"_tokens_\"].tolist()\n",
        "\n",
        "print(f\"Train: {len(train_seqs)}, Val: {len(val_seqs)}, Test: {len(test_seqs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0518586",
      "metadata": {},
      "source": [
        "## 5) Vocabulario y codificación\n",
        "Construimos el vocabulario a partir de los datos de entrenamiento y definimos la codificación de los tokens/acordes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e567dc67",
      "metadata": {},
      "source": [
        "### Tokens especiales\n",
        "\n",
        "Al construir el vocabulario incluimos **tokens especiales** que cumplen funciones clave durante el entrenamiento y la inferencia:\n",
        "\n",
        "- **`<pad>`** (*padding*): usado para rellenar secuencias hasta que todas tengan la misma longitud. Permite entrenar en *batch* sin que las secuencias más cortas generen errores.  \n",
        "- **`<unk>`** (*unknown*): representa cualquier token desconocido (acorde no visto en el entrenamiento). Esto evita que el modelo falle si aparece un símbolo nuevo.  \n",
        "- **`<bos>`** (*begin of sequence*): indica el inicio de una secuencia. Ayuda al modelo a aprender contextos desde el arranque.  \n",
        "- **`<eos>`** (*end of sequence*): marca el final de una secuencia. Fundamental para que el modelo sepa dónde detenerse al generar predicciones.\n",
        "\n",
        "Estos tokens forman parte del **alfabeto mínimo** que todo modelo de lenguaje necesita.  Son esenciales para manejar secuencias de longitud variable, gestionar vocabularios abiertos y proporcionar señales claras sobre la estructura de las secuencias durante el entrenamiento y la inferencia."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08cb6897",
      "metadata": {},
      "source": [
        "### ¿Por qué solo datos de entrenamiento para el vocabulario?\n",
        "\n",
        "**Principio fundamental: Evitar Data Leakage (filtración de datos)**\n",
        "\n",
        "Construimos el vocabulario **únicamente** con las secuencias de entrenamiento por estas razones críticas:\n",
        "\n",
        "1. **Evaluación realista**: Si incluyéramos tokens de validación/test en el vocabulario, el modelo tendría ventaja artificial al conocer todos los tokens posibles de antemano.\n",
        "\n",
        "2. **Generalización real**: En producción, el modelo encontrará acordes/progresiones nunca vistos. Usar solo datos de entrenamiento simula esta condición realista.\n",
        "\n",
        "3. **Prevención de sobreajuste**: Evita que el modelo se beneficie indirectamente de conocer la distribución completa de tokens del dataset.\n",
        "\n",
        "4. **Manejo de tokens desconocidos**: Los tokens que aparecen en val/test pero no en train se mapean a `<unk>`, lo cual es **intencional** y mide la robustez del modelo ante la incertidumbre.\n",
        "\n",
        "> **Nota**: Este es un estándar en ML que asegura que nuestras métricas reflejen el rendimiento real esperado en datos no vistos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9178d49f",
      "metadata": {},
      "source": [
        "### Guardado del tokenizador\n",
        "\n",
        "Guardamos el tokenizador `gru_tokenizer.json` (es decir, el diccionario que asigna tokens ↔︎ índices) por varias razones:\n",
        "\n",
        "1. **Reproducibilidad**: asegura que podamos volver a cargar el modelo y usar exactamente el mismo mapeo de tokens.  \n",
        "2. **Inferencia**: durante la predicción, necesitamos convertir acordes a índices y de vuelta a acordes.  \n",
        "3. **Compatibilidad**: si el tokenizador cambia, un modelo entrenado dejaría de ser válido.\n",
        "\n",
        "Este archivo almacenan dos diccionarios:\n",
        "- `stoi` (string → índice)  \n",
        "- `itos` (índice → string)  \n",
        "Este archivo queda en el directorio de trabajo junto a los pesos del modelo y se carga siempre antes de usar el modelo para predicciones.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c2802e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c2802e2",
        "outputId": "16aa75a7-dfc3-4302-9bd8-491fa1864fad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamaño del vocabulario: 86\n",
            "Tokenizer guardado en: lstm_tokenizer.json\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import json\n",
        "\n",
        "# Tokens especiales\n",
        "PAD, UNK, BOS, EOS = \"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"\n",
        "tokenizer_path = \"lstm_tokenizer.json\"  # Nombre del archivo del tokenizer\n",
        "\n",
        "def build_vocab(seqs, min_freq=1):\n",
        "    \"\"\"\n",
        "    Construye el vocabulario SOLO con secuencias de entrenamiento.\n",
        "    \n",
        "    IMPORTANTE: No incluir datos de validación/test para evitar data leakage.\n",
        "    Los tokens no vistos en entrenamiento se mapearán a <unk> durante la evaluación.\n",
        "    \"\"\"\n",
        "    c = Counter()\n",
        "    for s in seqs:\n",
        "        c.update(s)\n",
        "\n",
        "    # Crear vocabulario: tokens especiales + tokens frecuentes\n",
        "    vocab = [PAD, UNK, BOS, EOS] + [t for t,f in c.items() if f >= min_freq and t not in {PAD,UNK,BOS,EOS}]\n",
        "    stoi = {t:i for i,t in enumerate(vocab)}  # string to index\n",
        "    itos = {i:t for t,i in stoi.items()}     # index to string\n",
        "    return vocab, stoi, itos\n",
        "\n",
        "# 🎯 Construir vocabulario SOLO con datos de entrenamiento (evita data leakage)\n",
        "vocab, stoi, itos = build_vocab(train_seqs, min_freq=1)\n",
        "vocab_size = len(vocab)\n",
        "print(\"Tamaño del vocabulario:\", vocab_size)\n",
        "print(f\"📊 Tokens únicos encontrados en entrenamiento: {vocab_size - 4}\")  # -4 por tokens especiales\n",
        "\n",
        "# Guardar el tokenizer para uso posterior\n",
        "with open(tokenizer_path, \"w\") as f:\n",
        "    json.dump({\"vocab\": vocab}, f, ensure_ascii=False, indent=2)\n",
        "print(f\"Tokenizer guardado en: {tokenizer_path}\")\n",
        "\n",
        "def encode(seq, add_bos=True):\n",
        "    \"\"\"\n",
        "    Convierte una secuencia de tokens a índices numéricos.\n",
        "    Los tokens no vistos en entrenamiento se mapean automáticamente a <unk>.\n",
        "    \"\"\"\n",
        "    ids = [stoi[BOS]] if add_bos else []\n",
        "    ids += [stoi.get(t, stoi[UNK]) for t in seq]  # .get() maneja tokens desconocidos\n",
        "    return ids"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aa7271d",
      "metadata": {},
      "source": [
        "## 6) Dataset (context→next) y DataLoaders\n",
        "En este paso preparamos los datos para el entrenamiento:\n",
        "\n",
        "- **Dataset (context → next)**: cada ejemplo se construye como una secuencia de acordes (contexto) y el acorde que debe predecirse (siguiente).  \n",
        "- **DataLoader**: organiza el dataset en *batches* y lo entrega al modelo durante el entrenamiento.\n",
        "\n",
        "**Ventajas del DataLoader**:\n",
        "- Agrupa varios ejemplos en paralelo (*batching*), aprovechando mejor la GPU.  \n",
        "- Reordena los datos en cada época (*shuffle*), lo que mejora la generalización.  \n",
        "- Facilita el recorrido con bucles simples (`for batch in dataloader:`).  \n",
        "\n",
        "En resumen, el DataLoader hace más eficiente y ordenado el proceso de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "72490711",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72490711",
        "outputId": "31579165-45d2-4376-8e6a-4877f38e445e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Muestras de entrenamiento: 60979\n",
            "Muestras de validación: 7044\n",
            "Muestras de test: 7669\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Configuración del modelo y entrenamiento\n",
        "seq_len = 24\n",
        "batch_size = 128\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class NextTokenDataset(Dataset):\n",
        "    def __init__(self, sequences, seq_len):\n",
        "        self.samples = []\n",
        "        for seq in sequences:\n",
        "            ids = encode(seq, add_bos=True)\n",
        "            if len(ids) <= seq_len: continue\n",
        "            for i in range(seq_len, len(ids)):\n",
        "                self.samples.append((ids[i-seq_len:i], ids[i]))\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.samples[idx]\n",
        "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "train_data = NextTokenDataset(train_seqs, seq_len)\n",
        "val_data   = NextTokenDataset(val_seqs,   seq_len)\n",
        "test_data  = NextTokenDataset(test_seqs,  seq_len)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "val_loader   = DataLoader(val_data,   batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_data,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Muestras de entrenamiento: {len(train_data)}\")\n",
        "print(f\"Muestras de validación: {len(val_data)}\")\n",
        "print(f\"Muestras de test: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec1916a2",
      "metadata": {
        "id": "ec1916a2"
      },
      "source": [
        "## 7) Modelo LSTM\n",
        "\n",
        "En este caso definimos una red **LSTM** (*Long Short-Term Memory*), una variante de las RNN diseñada para capturar dependencias a largo plazo en secuencias.  \n",
        "La LSTM incorpora **puertas de entrada, olvido y salida**, lo que le permite retener información durante más pasos que una GRU o una RNN simple.  \n",
        "\n",
        "Esto hace que el modelo sea más potente en teoría para secuencias largas, aunque también más costoso en tiempo de entrenamiento y con mayor riesgo de sobreajuste en datasets pequeños."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b17ed09",
      "metadata": {
        "id": "6b17ed09"
      },
      "outputs": [],
      "source": [
        "#@title Definición del modelo\n",
        "import torch.nn as nn\n",
        "\n",
        "class ChordLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers,\n",
        "                                   batch_first=True, dropout=dropout if num_layers>1 else 0.0)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "    def forward(self, x):\n",
        "        e = self.emb(x)\n",
        "        o, _ = self.rnn(e)\n",
        "        return self.fc(self.dropout(o[:, -1, :]))\n",
        "    \n",
        "# Parámetros del modelo\n",
        "embedding_dim = 128\n",
        "hidden_size = 256\n",
        "num_layers = 2\n",
        "dropout = 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fffb0ff",
      "metadata": {
        "id": "7fffb0ff"
      },
      "source": [
        "## 8) Definimos el entrenamiento y las métricas (Top@K, MRR, PPL)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c0469ec",
      "metadata": {},
      "source": [
        "Durante el entrenamiento evaluamos el modelo con varias métricas:\n",
        "\n",
        "- **Top@K**: mide si el acorde correcto aparece entre las *K* predicciones más probables.  \n",
        "  - Ej: Top@1 = acierto exacto, Top@3 = acierto si está en las 3 primeras opciones.  \n",
        "\n",
        "- **MRR (Mean Reciprocal Rank)**: promedia la posición del acorde correcto en las predicciones.  \n",
        "  - Un valor alto significa que el modelo suele colocar la respuesta correcta en posiciones cercanas al inicio.  \n",
        "\n",
        "- **PPL (Perplexity)**: mide la “incertidumbre” del modelo. Valores bajos indican predicciones más seguras y consistentes.  \n",
        "\n",
        "Estas métricas combinadas nos dan una visión completa: precisión práctica (Top@K), calidad del ranking (MRR) y solidez estadística (PPL)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48525929",
      "metadata": {},
      "source": [
        "### Proceso de entrenamiento\n",
        "\n",
        "En el entrenamiento definimos tres componentes clave:\n",
        "\n",
        "- **GradScaler (AMP)**: activa el entrenamiento en precisión mixta en GPU. Esto acelera los cálculos y reduce memoria sin perder estabilidad numérica.  \n",
        "- **Optimizador (AdamW)**: actualiza los parámetros del modelo a partir de los gradientes. Incluye `weight_decay` como regularización para evitar sobreajuste.  \n",
        "- **Función de pérdida (CrossEntropyLoss)**: compara la predicción del modelo con el acorde correcto y guía el aprendizaje.\n",
        "\n",
        "El bucle de entrenamiento consiste en:  \n",
        "1. Pasar un *batch* por el modelo (*forward*).  \n",
        "2. Calcular la pérdida con `CrossEntropyLoss`.  \n",
        "3. Retropropagar los gradientes (*backward*).  \n",
        "4. Actualizar parámetros con AdamW (apoyado por GradScaler en GPU).  \n",
        "\n",
        "Este ciclo se repite durante varias épocas hasta que el modelo converge.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "15db18bc",
      "metadata": {
        "id": "15db18bc"
      },
      "outputs": [],
      "source": [
        "import math, time, os, torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def topk_metrics(logits, targets, ks=(1,3,5)):\n",
        "    out = {}\n",
        "    with torch.no_grad():\n",
        "        for k in ks:\n",
        "            topk = logits.topk(k, dim=-1).indices\n",
        "            out[f\"Top@{k}\"] = (topk == targets.unsqueeze(1)).any(dim=1).float().mean().item()\n",
        "        ranks = (logits.argsort(dim=-1, descending=True) == targets.unsqueeze(1)).nonzero(as_tuple=False)[:,1] + 1\n",
        "        out[\"MRR\"] = (1.0 / ranks.float()).mean().item()\n",
        "    return out\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total, n = 0.0, 0\n",
        "    agg = {\"Top@1\":0.0,\"Top@3\":0.0,\"Top@5\":0.0,\"MRR\":0.0}\n",
        "    with torch.no_grad():\n",
        "        for x,y in loader:\n",
        "            x,y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            b = x.size(0); total += loss.item()*b; n += b\n",
        "            m = topk_metrics(logits, y)\n",
        "            for k in agg: agg[k] += m[k]*b\n",
        "    for k in agg: agg[k] /= max(1,n)\n",
        "    return {\"loss\": total/max(1,n), \"ppl\": math.exp(total/max(1,n)), **agg}\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs, lr, weight_decay, grad_clip=1.0, amp=True, save_dir=\".\", save_name=\"best.pt\"):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(amp and device.type=='cuda'))\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    crit = torch.nn.CrossEntropyLoss()\n",
        "    best_mrr, best_path = -1.0, os.path.join(save_dir, save_name)\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train(); t0 = time.time()\n",
        "        for i,(x,y) in enumerate(train_loader,1):\n",
        "            x,y = x.to(device), y.to(device)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with torch.cuda.amp.autocast(enabled=(amp and device.type=='cuda')):\n",
        "                logits = model(x); loss = crit(logits,y)\n",
        "            scaler.scale(loss).backward()\n",
        "            if grad_clip is not None:\n",
        "                scaler.unscale_(opt)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "            scaler.step(opt); scaler.update()\n",
        "            if i % 100 == 0: print(f\"Ep{ep} step {i}/{len(train_loader)} loss {loss.item():.4f}\")\n",
        "        valm = evaluate(model, val_loader, crit)\n",
        "        print(f\"Epoch {ep} | val loss {valm['loss']:.4f} ppl {valm['ppl']:.2f} Top@1 {valm['Top@1']:.3f} Top@3 {valm['Top@3']:.3f} Top@5 {valm['Top@5']:.3f} MRR {valm['MRR']:.3f}\")\n",
        "        if valm[\"MRR\"] > best_mrr:\n",
        "            best_mrr = valm[\"MRR\"]\n",
        "            torch.save({\"model_state\": model.state_dict(), \"config\": {\"data_path\": data_path, \"sequence_col\": sequence_col, \"val_size\": val_size, \"test_size\": test_size, \"random_state\": random_state, \"seq_len\": seq_len, \"batch_size\": batch_size, \"epochs\": epochs, \"lr\": lr, \"weight_decay\": weight_decay, \"dropout\": dropout, \"embedding_dim\": embedding_dim, \"hidden_size\": hidden_size, \"num_layers\": num_layers, \"grad_clip\": grad_clip, \"amp\": amp, \"save_dir\": save_dir, \"save_name\": save_name, \"tokenizer_path\": tokenizer_path, \"min_seq_len\": min_seq_len}, \"stoi\": stoi, \"itos\": itos}, best_path)\n",
        "            print(\"🔥 Guardado best ->\", best_path, \"| MRR:\", best_mrr)\n",
        "    return best_mrr, best_path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8cc5fcc",
      "metadata": {
        "id": "f8cc5fcc"
      },
      "source": [
        "## 9) Entrenamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05d64822",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05d64822",
        "outputId": "e83b7394-b389-48a7-cc84-5353ad2f80c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3038281887.py:31: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(amp and device.type=='cuda'))\n",
            "/tmp/ipython-input-3038281887.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(amp and device.type=='cuda')):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ep1 step 100/476 loss 2.6424\n",
            "Ep1 step 200/476 loss 2.3758\n",
            "Ep1 step 300/476 loss 2.3740\n",
            "Ep1 step 400/476 loss 2.1818\n",
            "Epoch 1 | val loss 2.2173 ppl 9.18 Top@1 0.433 Top@3 0.666 Top@5 0.755 MRR 0.577\n",
            "🔥 Guardado best -> /content/models_lstm_v1/lstm_best.pt | MRR: 0.577345217986922\n",
            "Ep2 step 100/476 loss 2.3947\n",
            "Ep2 step 200/476 loss 1.9283\n",
            "Ep2 step 300/476 loss 2.0877\n",
            "Ep2 step 400/476 loss 2.1957\n",
            "Epoch 2 | val loss 2.1397 ppl 8.50 Top@1 0.450 Top@3 0.679 Top@5 0.767 MRR 0.591\n",
            "🔥 Guardado best -> /content/models_lstm_v1/lstm_best.pt | MRR: 0.5912135202460909\n",
            "Ep3 step 100/476 loss 1.7783\n",
            "Ep3 step 200/476 loss 1.8136\n",
            "Ep3 step 300/476 loss 2.2307\n",
            "Ep3 step 400/476 loss 2.0544\n",
            "Epoch 3 | val loss 2.1099 ppl 8.25 Top@1 0.457 Top@3 0.685 Top@5 0.772 MRR 0.597\n",
            "🔥 Guardado best -> /content/models_lstm_v1/lstm_best.pt | MRR: 0.5973487809461197\n",
            "Ep4 step 100/476 loss 2.0788\n",
            "Ep4 step 200/476 loss 1.7295\n",
            "Ep4 step 300/476 loss 1.7985\n",
            "Ep4 step 400/476 loss 1.8688\n",
            "Epoch 4 | val loss 2.0949 ppl 8.12 Top@1 0.456 Top@3 0.690 Top@5 0.775 MRR 0.598\n",
            "🔥 Guardado best -> /content/models_lstm_v1/lstm_best.pt | MRR: 0.5979488188404037\n",
            "Ep5 step 100/476 loss 2.1388\n",
            "Ep5 step 200/476 loss 1.9473\n",
            "Ep5 step 300/476 loss 1.8147\n",
            "Ep5 step 400/476 loss 1.7077\n",
            "Epoch 5 | val loss 2.1095 ppl 8.24 Top@1 0.459 Top@3 0.692 Top@5 0.770 MRR 0.599\n",
            "🔥 Guardado best -> /content/models_lstm_v1/lstm_best.pt | MRR: 0.5991201563899589\n",
            "Ep6 step 100/476 loss 1.7229\n",
            "Ep6 step 200/476 loss 1.5143\n",
            "Ep6 step 300/476 loss 1.7070\n",
            "Ep6 step 400/476 loss 1.8254\n",
            "Epoch 6 | val loss 2.1275 ppl 8.39 Top@1 0.457 Top@3 0.690 Top@5 0.770 MRR 0.598\n",
            "MRR: 0.5991201563899589 | Modelo guardado en: /content/models_lstm_v1/lstm_best.pt\n"
          ]
        }
      ],
      "source": [
        "#@title Train\n",
        "import torch, os, json\n",
        "\n",
        "# Parámetros de entrenamiento\n",
        "epochs = 6\n",
        "lr = 2e-3\n",
        "weight_decay = 1e-4\n",
        "grad_clip = 1.0\n",
        "amp = True\n",
        "\n",
        "# Configuración de guardado\n",
        "save_dir = \"/content/models_lstm_v1\"\n",
        "save_name = \"lstm_best.pt\"\n",
        "\n",
        "torch.manual_seed(random_state)\n",
        "\n",
        "# Instanciar el modelo\n",
        "model = ChordLSTM(vocab_size=len(vocab), embedding_dim=embedding_dim,\n",
        "                          hidden_size=hidden_size, num_layers=num_layers,\n",
        "                          dropout=dropout).to(device)\n",
        "\n",
        "# Entrenar el modelo\n",
        "best_mrr, best_path = train_model(model, train_loader, val_loader, epochs, lr, weight_decay,\n",
        "                                  grad_clip=grad_clip, amp=amp, save_dir=save_dir, save_name=save_name)\n",
        "\n",
        "print(f\"MRR: {best_mrr} | Modelo guardado en: {best_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7880ded",
      "metadata": {},
      "source": [
        "## 10) Evaluación en Test\n",
        "Una vez entrenado el modelo, lo evaluamos en el conjunto de **test**:\n",
        "\n",
        "- Se usa el mismo *pipeline* que en entrenamiento (forward + cálculo de pérdida y métricas).  \n",
        "- **No** se actualizan los parámetros del modelo:  \n",
        "  - Se desactiva el cálculo de gradientes (`torch.no_grad()`).  \n",
        "  - Se mantiene el modelo en modo evaluación (`model.eval()`), lo que desactiva capas como *dropout*.  \n",
        "\n",
        "De este modo obtenemos métricas (Top@K, MRR, PPL) que reflejan el rendimiento real del modelo en datos **nunca vistos**, asegurando una evaluación justa y sin fugas de información.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5277126c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5277126c",
        "outputId": "87619861-e93f-4fe6-c9d7-7ca1102100e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Métricas finales en Test:\n",
            "  • Loss: 2.1916\n",
            "  • Perplexity: 8.95\n",
            "  • Top@1: 0.434\n",
            "  • Top@3: 0.679\n",
            "  • Top@5: 0.760\n",
            "  • MRR: 0.580\n"
          ]
        }
      ],
      "source": [
        "#@title Test\n",
        "import torch, os\n",
        "\n",
        "# Cargar el mejor modelo guardado\n",
        "model_path = os.path.join(save_dir, save_name)\n",
        "ckpt = torch.load(model_path, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model_state\"])\n",
        "\n",
        "\n",
        "# Evaluar en conjunto de test\n",
        "test_metrics = evaluate(model, test_loader, torch.nn.CrossEntropyLoss())\n",
        "\n",
        "print(\"📊 Métricas finales en Test:\")\n",
        "print(f\"  • Loss: {test_metrics['loss']:.4f}\")\n",
        "print(f\"  • Perplexity: {test_metrics['ppl']:.2f}\")\n",
        "print(f\"  • Top@1: {test_metrics['Top@1']:.3f}\")\n",
        "print(f\"  • Top@3: {test_metrics['Top@3']:.3f}\")\n",
        "print(f\"  • Top@5: {test_metrics['Top@5']:.3f}\")\n",
        "print(f\"  • MRR: {test_metrics['MRR']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0b19a79",
      "metadata": {},
      "source": [
        "## 11) Conclusiones\n",
        "\n",
        "Los resultados muestran que el modelo **LSTM** obtiene métricas casi idénticas a la GRU y muy similares al baseline Kneser–Ney:  \n",
        "\n",
        "| Modelo   | Top@1 | Top@3 | Top@5 | MRR   | PPL   |\n",
        "|----------|-------|-------|-------|-------|-------|\n",
        "| KN v2    | 0.433 | 0.661 | 0.741 | 0.573 |   –   |\n",
        "| GRU v2   | 0.432 | 0.678 | 0.762 | 0.580 | 8.99  |\n",
        "| LSTM v2  | 0.434 | 0.679 | 0.760 | 0.580 | 8.95  |\n",
        "\n",
        "Aunque la LSTM logra un resultado **ligeramente superior en Top@1 y Top@3**, la diferencia es mínima y no supone una mejora sustancial sobre GRU o KN.  \n",
        "\n",
        "Esto confirma que, en el dominio de la armonía funcional, donde la predicción depende sobre todo de los últimos acordes del contexto, **los modelos complejos como LSTM no aportan una ventaja clara** frente a un modelo estadístico bien afinado.  \n",
        "\n",
        "Siguiendo el principio de parsimonia, consideramos que el baseline **KN con GridSearch** sigue siendo la opción más estable, interpretable y eficiente para un despliegue en producción.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "510af377",
      "metadata": {
        "id": "510af377"
      },
      "source": [
        "## 12) Función para predicciones \n",
        "`predict_next(context, k=5)`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0a41674",
      "metadata": {},
      "source": [
        "Esta función facilita el uso del modelo desde el punto de vista del usuario: dado un **contexto** de acordes, devuelve las **K** sugerencias más probables para el siguiente acorde.\n",
        "\n",
        "**Qué hace paso a paso:**\n",
        "1. **Preprocesado del contexto**  \n",
        "   - Normaliza la entrada (lista de tokens) y añade marcadores si procede (p. ej., `<bos>` al inicio).  \n",
        "   - Convierte cada token a índice usando el diccionario `stoi` (los desconocidos pasan a `<unk>`).\n",
        "\n",
        "2. **Inferencia con el modelo**  \n",
        "   - Pone el modelo en modo evaluación: `model.eval()` y `torch.no_grad()`.  \n",
        "   - Pasa la secuencia al modelo (en el dispositivo correcto: CPU/GPU).  \n",
        "   - Obtiene los **logits** para el siguiente paso y aplica `softmax` para convertir a probabilidades.\n",
        "\n",
        "3. **Selección Top-K**  \n",
        "   - Toma los **K** índices con mayor probabilidad (`topk`).  \n",
        "   - Convierte esos índices a tokens legibles usando `itos`.\n",
        "\n",
        "4. **Salida legible**  \n",
        "   - Devuelve una lista de pares `(token_predicho, probabilidad)` ordenada de mayor a menor.\n",
        "\n",
        "**Notas prácticas:**\n",
        "- Si el modelo se entrenó con tokens funcionales (p. ej., `I`, `ii`, `V7`), aquí se devuelven esos mismos tokens.  \n",
        "- Para entradas con símbolos no vistos, el mapeo a `<unk>` evita errores y mantiene la robustez.  \n",
        "- Puede incorporarse filtrado opcional (p. ej., máscaras por tonalidad o evitar repeticiones) **después** de obtener `topk`.\n",
        "\n",
        "En resumen, `predict_next` es el puente entre el usuario y el modelo: traduce el contexto a índices, ejecuta la inferencia y devuelve predicciones claras y ordenadas por probabilidad.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "165bfb26",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "165bfb26",
        "outputId": "5f192ea8-7d1d-4870-8f8c-7cfa3a18596e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context: ['III', 'III', '#IV', '#IV', 'bII', 'bII', 'natIII', 'natIII', 'III', '#IV', 'IV', 'VI', 'V', 'bVII', 'natVI', 'I', 'II', 'VII', 'VI', 'IV', 'III', 'I', 'VII', 'natVI']\n",
            "Pred: [('IV', 0.1506534218788147), ('II', 0.13384999334812164), ('natVI', 0.07967973500490189), ('VI', 0.07328757643699646), ('bVII', 0.06727532297372818)]\n"
          ]
        }
      ],
      "source": [
        "#@title predict_next()\n",
        "import torch.nn.functional as F\n",
        "def predict_next(context_tokens, k=5):\n",
        "    model.eval()\n",
        "    ids = [stoi.get(t, stoi[\"<unk>\"]) for t in context_tokens]\n",
        "    if len(ids) < seq_len:\n",
        "        ids = [stoi[\"<bos>\"]] * (seq_len - len(ids)) + ids\n",
        "    else:\n",
        "        ids = ids[-seq_len:]\n",
        "    import torch\n",
        "    x = torch.tensor(ids, dtype=torch.long, device=device).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)\n",
        "        probs = F.softmax(logits[0], dim=-1)\n",
        "        topk = torch.topk(probs, k)\n",
        "        return [(itos[i], float(p)) for i,p in zip(topk.indices.tolist(), topk.values.tolist())]\n",
        "\n",
        "# Ejemplo rápido (si hay datos)\n",
        "if len(train_seqs):\n",
        "    ctx = train_seqs[0][:seq_len]\n",
        "    print(\"Context:\", ctx)\n",
        "    print(\"Pred:\", predict_next(ctx, k=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8cn_1P6r3d2",
      "metadata": {
        "id": "e8cn_1P6r3d2"
      },
      "source": [
        "## 13) Inferencia incremental (quick test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "jQ2Jh9KKrnti",
      "metadata": {
        "id": "jQ2Jh9KKrnti"
      },
      "outputs": [],
      "source": [
        "# @title Stateful predictor para uso interactivo paso a paso\n",
        "import torch, torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "class StatefulPredictor:\n",
        "    def __init__(self, model, stoi, itos, device, start_with_bos=True):\n",
        "        self.model, self.stoi, self.itos = model.eval(), stoi, itos\n",
        "        self.device = device\n",
        "        self.start_with_bos = start_with_bos\n",
        "        self.h = None  # (h,c) en LSTM; h en GRU\n",
        "\n",
        "    def reset(self):\n",
        "        self.h = None\n",
        "        if self.start_with_bos and \"<bos>\" in self.stoi:\n",
        "            self._step(\"<bos>\")  # alinear con el entrenamiento\n",
        "\n",
        "    def _step(self, token):\n",
        "        tid = torch.tensor([[self.stoi.get(token, self.stoi[\"<unk>\"])]], device=self.device)\n",
        "        e = self.model.emb(tid)                             # (1,1,E)\n",
        "        # Funciona igual para LSTM y GRU\n",
        "        out, self.h = self.model.rnn(e, self.h)            # out: (1,1,H)\n",
        "        logits = self.model.fc(out[:, -1, :])              # (1,V)\n",
        "        return logits[0]\n",
        "\n",
        "    def add(self, token, k=5):\n",
        "        \"\"\"Introduce el último acorde del contexto y devuelve predicción para el SIGUIENTE.\"\"\"\n",
        "        logits = self._step(token)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        topk = torch.topk(probs, k)\n",
        "        return [(self.itos[i.item()], float(p.item())) for i,p in zip(topk.indices, topk.values)]\n",
        "\n",
        "    def suggest(self, context_tokens, k=5):\n",
        "        \"\"\"Da una predicción para el siguiente acorde tras un contexto completo.\"\"\"\n",
        "        self.reset()\n",
        "        preds = None\n",
        "        for t in context_tokens:\n",
        "            preds = self.add(t, k=k)  # la última llamada produce la sugerencia para el siguiente\n",
        "        return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "S1nHo4t3gwY9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1nHo4t3gwY9",
        "outputId": "fd463b4f-3539-4d3b-9ae6-42fba4eef953"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('V', 0.2221945822238922),\n",
              " ('ii', 0.10717958956956863),\n",
              " ('V/V', 0.05786261335015297),\n",
              " ('II7', 0.04348348453640938),\n",
              " ('V7', 0.038056351244449615)]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sp = StatefulPredictor(model, stoi, itos, device)\n",
        "sp.suggest([\"ii\",\"V\"], k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "__5FJ5ZnXwkC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__5FJ5ZnXwkC",
        "outputId": "b984a295-6129-48d2-e760-ce236fb12cd3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('V7', 0.6182572245597839),\n",
              " ('ii', 0.084834985435009),\n",
              " ('i', 0.06577493250370026),\n",
              " ('vi', 0.02957976795732975),\n",
              " ('v', 0.02462872862815857)]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sp.suggest([\"i\",\"vi\", 'ii'], k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "qfVOVw4JYSJN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfVOVw4JYSJN",
        "outputId": "7d86889d-828d-4bc9-8517-567eabb84b2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('i', 0.16813920438289642),\n",
              " ('v', 0.08122055232524872),\n",
              " ('bII7', 0.07875661551952362),\n",
              " ('III', 0.06942159682512283),\n",
              " ('bii', 0.05412622168660164)]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sp.suggest([\"i\",\"bvi\"], k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53afbbdb",
      "metadata": {
        "id": "53afbbdb"
      },
      "source": [
        "### Roadmap\n",
        "- Ajuste de hiperparámetros (Ranadom Search)\n",
        "- Re-ranking suave para evitar repes y favorecer cadencias.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
