{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36e6ce9a",
   "metadata": {
    "id": "36e6ce9a"
   },
   "source": [
    "**Nota**: Se recomienda ejecutar este notebook en Google Colab para asegurar la compatibilidad y evitar problemas de dependencias. El entrenamiento de los modelos requiere una cantidad significativa de memoria RAM y potencia de cómputo, que puede no estar disponible en todos los entornos locales.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/antoniotrapote/chord-prediction-tfm/blob/main/anexos/notebooks/03_modelado/04_modelo_lstm.ipynb)\n",
    "[![View on GitHub](https://img.shields.io/badge/View_on-GitHub-black?logo=github)](https://github.com/antoniotrapote/chord-prediction-tfm/blob/main/anexos/notebooks/03_modelado/04_modelo_lstm.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00436936",
   "metadata": {
    "id": "00436936"
   },
   "source": [
    "# Long Short-Term Memory (LSTM) model - PyTorch\n",
    "\n",
    "Hemos utilizado PyTorch para implementar y entrenar un modelo de red neuronal recurrente basado en Long Short-Term Memory (LSTM) para la predicción de acordes en secuencias musicales.\n",
    "\n",
    "El último dataset utilizado fue `songdb_funcional_v4`\n",
    "\n",
    "Contenido del notebook:\n",
    "1. Entorno (Colab) - comprobación de versiones y semillas\n",
    "2. Descarga del dataset desde GitHub\n",
    "3. Carga y tokenización del dataset\n",
    "\n",
    "## 1) Entorno (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "C38G_ux7fg8i",
   "metadata": {
    "executionInfo": {
     "elapsed": 7886,
     "status": "ok",
     "timestamp": 1756231091159,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "C38G_ux7fg8i"
   },
   "outputs": [],
   "source": [
    "#@title Semillas y determinismo\n",
    "import random, os, numpy as np, torch\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d96011",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1756231091177,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "48d96011",
    "outputId": "264304ed-d275-4f29-e637-959167ba41f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
      "PyTorch: 2.8.0+cu126\n",
      "CUDA disponible: True\n",
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#@title Comprobar GPU/versions\n",
    "import sys, torch\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"⚠️ Activa GPU: Runtime ▶ Change runtime type ▶ GPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec304546",
   "metadata": {
    "id": "ec304546"
   },
   "source": [
    "## 2) Traer el CSV desde GitHub\n",
    "Descargamos directamente el dataset procesado desde el repositorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7eb715",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "executionInfo": {
     "elapsed": 23603,
     "status": "ok",
     "timestamp": 1756231114818,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "9e7eb715",
    "outputId": "1907a388-bbe5-4079-b379-121d1efce8dd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-c12001dd-4a2b-4e19-8d14-883388c70d65\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-c12001dd-4a2b-4e19-8d14-883388c70d65\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving songdb_funcional_v4.csv to songdb_funcional_v4.csv\n",
      "Subido: ['songdb_funcional_v4.csv']\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Configuración para descargar el dataset desde GitHub\n",
    "USER = \"antoniotrapote\"\n",
    "REPO = \"chord-prediction-tfm\"\n",
    "BRANCH = \"main\"\n",
    "PATH_IN_REPO = \"anexos/data/songdb_funcional_v4.csv\"\n",
    "URL = f\"https://raw.githubusercontent.com/{USER}/{REPO}/{BRANCH}/{PATH_IN_REPO}\"\n",
    "\n",
    "# Ruta local donde guardar el archivo\n",
    "data_path = \"/content/songdb_funcional_v4.csv\"\n",
    "\n",
    "# Descargar el archivo CSV desde GitHub\n",
    "urllib.request.urlretrieve(URL, data_path)\n",
    "print(f\"Dataset descargado en: {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4a6a9c",
   "metadata": {
    "id": "ce4a6a9c"
   },
   "source": [
    "## 3) Cargar CSV y tokenizar (whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23512052",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "executionInfo": {
     "elapsed": 477,
     "status": "ok",
     "timestamp": 1756231117479,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "23512052",
    "outputId": "a8ad7241-9cc7-4058-8790-37ccd24975e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas totales: 2613\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"print(\\\"Filas tras filtro min_seq_len:\\\", len(df))\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"funcional_prog\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"vi #iv\\u00f8 V/III V/VI vi IV ii V7 iii vi ii V7 I IV7 vii\\u00f8 V/VI vi #iv\\u00f8 V/III V/VI vi IV ii V7 iii vi ii V7 I V7 I V/II ii ii V7 I V/II ii V7 I V/VI vi #iv\\u00f8 V/III V/VI vi IV ii V7 iii vi ii V7 I V7 I III7\",\n          \"VII VII I vi ii V7 VII VII I vi ii V7 I IV #ivo I iii vi V/V V7 VII VII I vi ii V7 VII VII I vi ii V7 I IV #ivo I ii V7 I I vi bvi v V/IV IV IV vi bvi v V/IV IV IV vii bvii vi V/V V V ii V7 ii V7 VII VII I vi ii V7 VII VII I vi ii V7 I IV #ivo I iii vi V/V V7 iii IV iii IV iii ii ii V7 I V7\",\n          \"i VI V/V V7 i VI V/V V7 i VI ii\\u00f8 V7 i VI ii\\u00f8 V7 i i Vsub/V V7 iv iv ii\\u00f8 bII7 iv bII v\\u00f8 V/IV iv bII v\\u00f8 V/IV iv bII v\\u00f8 V/IV iv Vsub/II ii\\u00f8 V7 i VI ii\\u00f8 V7 i VI ii\\u00f8 V7 i i Vsub/V V7 iv iv ii\\u00f8 bII7 bII bII\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-0cb95577-b05b-4157-ab1e-510501ae1522\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>funcional_prog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vi #ivø V/III V/VI vi IV ii V7 iii vi ii V7 I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VII VII I vi ii V7 VII VII I vi ii V7 I IV #iv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i VI V/V V7 i VI V/V V7 i VI iiø V7 i VI iiø V...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0cb95577-b05b-4157-ab1e-510501ae1522')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-0cb95577-b05b-4157-ab1e-510501ae1522 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-0cb95577-b05b-4157-ab1e-510501ae1522');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-38a7b579-3a4b-4a3e-816b-6c37c3213e03\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-38a7b579-3a4b-4a3e-816b-6c37c3213e03')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-38a7b579-3a4b-4a3e-816b-6c37c3213e03 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                      funcional_prog\n",
       "0  vi #ivø V/III V/VI vi IV ii V7 iii vi ii V7 I ...\n",
       "1  VII VII I vi ii V7 VII VII I vi ii V7 I IV #iv...\n",
       "2  i VI V/V V7 i VI V/V V7 i VI iiø V7 i VI iiø V..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas tras filtro min_seq_len: 2612\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, ast, re\n",
    "\n",
    "# Parámetros de filtrado\n",
    "sequence_col = \"funcional_prog\"  # Columna que contiene las secuencias de acordes\n",
    "min_seq_len = 8  # Ignorar secuencias muy cortas\n",
    "\n",
    "# Cargar el dataset\n",
    "df = pd.read_csv(data_path)\n",
    "assert sequence_col in df.columns, f\"Columna {sequence_col} no encontrada en el CSV.\"\n",
    "print(\"Filas totales:\", len(df))\n",
    "display(df[[sequence_col]].head(3))\n",
    "\n",
    "def parse_tokens_simple(s: str):\n",
    "    \"\"\"Convierte la secuencia de acordes en una lista de tokens\"\"\"\n",
    "    if isinstance(s, str) and s.strip().startswith(\"[\") and s.strip().endswith(\"]\"):\n",
    "        try:\n",
    "            lst = ast.literal_eval(s)\n",
    "            if isinstance(lst, list):\n",
    "                return [str(t) for t in lst]\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    # Normaliza separadores de compás y saltos de línea a espacios\n",
    "    s = str(s).replace(\"|\", \" \").replace(\"\\n\", \" \")\n",
    "    toks = [t for t in re.findall(r\"\\S+\", s) if t.strip()]\n",
    "    return toks\n",
    "\n",
    "# Tokenizar y filtrar secuencias muy cortas\n",
    "df[\"_tokens_\"] = df[sequence_col].apply(parse_tokens_simple)\n",
    "df = df[df[\"_tokens_\"].apply(len) >= min_seq_len].reset_index(drop=True)\n",
    "print(f\"Filas tras filtro min_seq_len >= {min_seq_len}:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395c628f",
   "metadata": {
    "id": "395c628f"
   },
   "source": [
    "## 4) Split train/val/test (simple, por filas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd427ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1016,
     "status": "ok",
     "timestamp": 1756231118500,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "6bd427ae",
    "outputId": "70336aa7-1e9b-4938-e153-1573660209ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2089 261 262\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Parámetros de división del dataset\n",
    "val_size = 0.10     # 10% para validación\n",
    "test_size = 0.10    # 10% para test\n",
    "random_state = 42   # Semilla para reproducibilidad\n",
    "\n",
    "# Dividir en train/val/test\n",
    "train_df, tmp_df = train_test_split(df, test_size=val_size+test_size, random_state=random_state, shuffle=True)\n",
    "rel_test = test_size / (val_size + test_size) if (val_size + test_size) > 0 else 0.5\n",
    "val_df, test_df = train_test_split(tmp_df, test_size=rel_test, random_state=random_state, shuffle=True)\n",
    "\n",
    "# Extraer las secuencias tokenizadas\n",
    "train_seqs = train_df[\"_tokens_\"].tolist()\n",
    "val_seqs   = val_df[\"_tokens_\"].tolist()\n",
    "test_seqs  = test_df[\"_tokens_\"].tolist()\n",
    "\n",
    "print(f\"Train: {len(train_seqs)}, Val: {len(val_seqs)}, Test: {len(test_seqs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525bbdae",
   "metadata": {
    "id": "525bbdae"
   },
   "source": [
    "## 5) Vocabulario y codificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2802e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1756231118507,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "6c2802e2",
    "outputId": "0a753895-c6e6-4971-857f-b77e18ed7324"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 86\n"
     ]
    }
   ],
   "source": [
    "# Configuración del vocabulario\n",
    "min_count = 4  # Frecuencia mínima para incluir un token en el vocabulario\n",
    "\n",
    "# Construir vocabulario desde train_seqs\n",
    "vocab = build_vocab_from_list(train_seqs, min_count=min_count)\n",
    "\n",
    "# Añadir tokens especiales\n",
    "vocab.add_token(\"[PAD]\")  \n",
    "vocab.add_token(\"[UNK]\")  \n",
    "vocab.add_token(\"[SOS]\")  \n",
    "vocab.add_token(\"[EOS]\")  \n",
    "\n",
    "# Configurar token por defecto\n",
    "vocab.set_default_index(vocab[\"[UNK]\"])\n",
    "\n",
    "print(f\"Tamaño del vocabulario: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969ecf11",
   "metadata": {
    "id": "969ecf11"
   },
   "source": [
    "## 6) Dataset (context→next) y DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72490711",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1756231118709,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "72490711",
    "outputId": "b7dde1b6-4231-475e-b1ce-6f10d1f2e889"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60979, 7044, 7669)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class NextTokenDataset(Dataset):\n",
    "    def __init__(self, sequences, seq_len):\n",
    "        self.samples = []\n",
    "        for seq in sequences:\n",
    "            ids = encode(seq, add_bos=True)\n",
    "            if len(ids) <= seq_len: continue\n",
    "            for i in range(seq_len, len(ids)):\n",
    "                self.samples.append((ids[i-seq_len:i], ids[i]))\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.samples[idx]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "train_data = NextTokenDataset(train_seqs, cfg.seq_len)\n",
    "val_data   = NextTokenDataset(val_seqs,   cfg.seq_len)\n",
    "test_data  = NextTokenDataset(test_seqs,  cfg.seq_len)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=cfg.batch_size, shuffle=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_data,   batch_size=cfg.batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_data,  batch_size=cfg.batch_size, shuffle=False)\n",
    "\n",
    "len(train_data), len(val_data), len(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1916a2",
   "metadata": {
    "id": "ec1916a2"
   },
   "source": [
    "## 7) Modelo LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b17ed09",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1756231118713,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "6b17ed09"
   },
   "outputs": [],
   "source": [
    "# Configuración del modelo LSTM\n",
    "vocab_size = len(vocab)     # Tamaño del vocabulario\n",
    "embed_dim = 256             # Dimensión de los embeddings\n",
    "hidden_dim = 512            # Dimensión de la capa oculta del LSTM\n",
    "num_layers = 2              # Número de capas LSTM\n",
    "dropout = 0.2               # Dropout para regularización\n",
    "\n",
    "# Crear el modelo\n",
    "model = LSTMModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "# Mover el modelo a GPU si está disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Mostrar información del modelo\n",
    "print(f\"Modelo creado con {sum(p.numel() for p in model.parameters()):,} parámetros\")\n",
    "print(f\"Dispositivo: {device}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fffb0ff",
   "metadata": {
    "id": "7fffb0ff"
   },
   "source": [
    "## 8) Entrenamiento y métricas (Top@K, MRR, PPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15db18bc",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1756231118717,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "15db18bc"
   },
   "outputs": [],
   "source": [
    "import math, time, os, torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def topk_metrics(logits, targets, ks=(1,3,5)):\n",
    "    out = {}\n",
    "    with torch.no_grad():\n",
    "        for k in ks:\n",
    "            topk = logits.topk(k, dim=-1).indices\n",
    "            out[f\"Top@{k}\"] = (topk == targets.unsqueeze(1)).any(dim=1).float().mean().item()\n",
    "        ranks = (logits.argsort(dim=-1, descending=True) == targets.unsqueeze(1)).nonzero(as_tuple=False)[:,1] + 1\n",
    "        out[\"MRR\"] = (1.0 / ranks.float()).mean().item()\n",
    "    return out\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total, n = 0.0, 0\n",
    "    agg = {\"Top@1\":0.0,\"Top@3\":0.0,\"Top@5\":0.0,\"MRR\":0.0}\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            b = x.size(0); total += loss.item()*b; n += b\n",
    "            m = topk_metrics(logits, y)\n",
    "            for k in agg: agg[k] += m[k]*b\n",
    "    for k in agg: agg[k] /= max(1,n)\n",
    "    return {\"loss\": total/max(1,n), \"ppl\": math.exp(total/max(1,n)), **agg}\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, lr, weight_decay, grad_clip=1.0, amp=True, save_dir=\".\", save_name=\"best.pt\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(amp and device.type=='cuda'))\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    crit = torch.nn.CrossEntropyLoss()\n",
    "    best_mrr, best_path = -1.0, os.path.join(save_dir, save_name)\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train(); t0 = time.time()\n",
    "        for i,(x,y) in enumerate(train_loader,1):\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=(amp and device.type=='cuda')):\n",
    "                logits = model(x); loss = crit(logits,y)\n",
    "            scaler.scale(loss).backward()\n",
    "            if grad_clip is not None:\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            scaler.step(opt); scaler.update()\n",
    "            if i % 100 == 0: print(f\"Ep{ep} step {i}/{len(train_loader)} loss {loss.item():.4f}\")\n",
    "        valm = evaluate(model, val_loader, crit)\n",
    "        print(f\"Epoch {ep} | val loss {valm['loss']:.4f} ppl {valm['ppl']:.2f} Top@1 {valm['Top@1']:.3f} Top@3 {valm['Top@3']:.3f} Top@5 {valm['Top@5']:.3f} MRR {valm['MRR']:.3f}\")\n",
    "        if valm[\"MRR\"] > best_mrr:\n",
    "            best_mrr = valm[\"MRR\"]\n",
    "            torch.save({\"model_state\": model.state_dict(), \"config\": dict(vars(cfg)), \"stoi\": stoi, \"itos\": itos}, best_path)\n",
    "            print(\"🔥 Guardado best ->\", best_path, \"| MRR:\", best_mrr)\n",
    "    return best_mrr, best_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cc5fcc",
   "metadata": {
    "id": "f8cc5fcc"
   },
   "source": [
    "## 9) Entrenar LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d64822",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42169,
     "status": "ok",
     "timestamp": 1756231160902,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "05d64822",
    "outputId": "22430c52-677f-461b-d304-e2515d570e31"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-717191719.py:31: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(amp and device.type=='cuda'))\n",
      "/tmp/ipython-input-717191719.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(amp and device.type=='cuda')):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep1 step 100/476 loss 2.2384\n",
      "Ep1 step 200/476 loss 2.3146\n",
      "Ep1 step 300/476 loss 2.0664\n",
      "Ep1 step 400/476 loss 2.2425\n",
      "Epoch 1 | val loss 2.1566 ppl 8.64 Top@1 0.437 Top@3 0.680 Top@5 0.767 MRR 0.584\n",
      "🔥 Guardado best -> /content/models_lstm_v1/lstm_best.pt | MRR: 0.5841175761837502\n",
      "Ep2 step 100/476 loss 2.2561\n",
      "Ep2 step 200/476 loss 1.9753\n",
      "Ep2 step 300/476 loss 2.1835\n",
      "Ep2 step 400/476 loss 1.9502\n",
      "Epoch 2 | val loss 2.0980 ppl 8.15 Top@1 0.453 Top@3 0.691 Top@5 0.774 MRR 0.597\n",
      "🔥 Guardado best -> /content/models_lstm_v1/lstm_best.pt | MRR: 0.5970214201197713\n",
      "Ep3 step 100/476 loss 2.3348\n",
      "Ep3 step 200/476 loss 1.9495\n",
      "Ep3 step 300/476 loss 1.7481\n",
      "Ep3 step 400/476 loss 1.7912\n",
      "Epoch 3 | val loss 2.0810 ppl 8.01 Top@1 0.460 Top@3 0.695 Top@5 0.776 MRR 0.602\n",
      "🔥 Guardado best -> /content/models_lstm_v1/lstm_best.pt | MRR: 0.6020486079651411\n",
      "Ep4 step 100/476 loss 1.8587\n",
      "Ep4 step 200/476 loss 1.7421\n",
      "Ep4 step 300/476 loss 1.8021\n",
      "Ep4 step 400/476 loss 2.1285\n",
      "Epoch 4 | val loss 2.0956 ppl 8.13 Top@1 0.452 Top@3 0.693 Top@5 0.779 MRR 0.597\n",
      "Ep5 step 100/476 loss 1.7164\n",
      "Ep5 step 200/476 loss 1.8218\n",
      "Ep5 step 300/476 loss 1.6124\n",
      "Ep5 step 400/476 loss 1.7764\n",
      "Epoch 5 | val loss 2.1347 ppl 8.45 Top@1 0.457 Top@3 0.685 Top@5 0.772 MRR 0.597\n",
      "Ep6 step 100/476 loss 1.4832\n",
      "Ep6 step 200/476 loss 1.5518\n",
      "Ep6 step 300/476 loss 1.6740\n",
      "Ep6 step 400/476 loss 1.4452\n",
      "Epoch 6 | val loss 2.1611 ppl 8.68 Top@1 0.445 Top@3 0.687 Top@5 0.771 MRR 0.591\n",
      "Best MRR: 0.6020486079651411 | path: /content/models_lstm_v1/lstm_best.pt\n"
     ]
    }
   ],
   "source": [
    "# Configuración del entrenamiento\n",
    "seq_len = 24               # Longitud de las secuencias de entrada\n",
    "batch_size = 128           # Tamaño del batch\n",
    "epochs = 6                 # Número de épocas\n",
    "lr = 2e-3                  # Learning rate\n",
    "weight_decay = 5e-4        # Weight decay para regularización\n",
    "\n",
    "# Tokens especiales\n",
    "pad_idx = vocab[\"[PAD]\"]\n",
    "sos_idx = vocab[\"[SOS]\"]\n",
    "eos_idx = vocab[\"[EOS]\"]\n",
    "\n",
    "# Crear los dataloaders\n",
    "train_loader = create_dataloader(\n",
    "    train_seqs, vocab, seq_len=seq_len, batch_size=batch_size, \n",
    "    pad_idx=pad_idx, sos_idx=sos_idx, eos_idx=eos_idx, shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader(\n",
    "    val_seqs, vocab, seq_len=seq_len, batch_size=batch_size, \n",
    "    pad_idx=pad_idx, sos_idx=sos_idx, eos_idx=eos_idx, shuffle=False\n",
    ")\n",
    "\n",
    "# Configurar optimizador y función de pérdida\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "print(f\"Configuración del entrenamiento:\")\n",
    "print(f\"  - Secuencias de entrada: {seq_len}\")\n",
    "print(f\"  - Batch size: {batch_size}\")\n",
    "print(f\"  - Épocas: {epochs}\")\n",
    "print(f\"  - Learning rate: {lr}\")\n",
    "print(f\"  - Weight decay: {weight_decay}\")\n",
    "print(f\"  - Train batches: {len(train_loader)}\")\n",
    "print(f\"  - Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c28d6e7",
   "metadata": {
    "id": "0c28d6e7"
   },
   "source": [
    "## 10) Evaluación en Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5277126c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1756231161512,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "5277126c",
    "outputId": "a1b00d77-b7a9-435f-8178-67bcaf8af4ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: {'loss': 2.169651968660403, 'ppl': 8.755236413779173, 'Top@1': 0.4305646108755591, 'Top@3': 0.6828791240480612, 'Top@5': 0.7668535665390716, 'MRR': 0.580885472159635}\n"
     ]
    }
   ],
   "source": [
    "# Configuración de la evaluación\n",
    "top_k = 10  # Top-k para las métricas\n",
    "\n",
    "# Evaluar el modelo entrenado\n",
    "print(\"Evaluando modelo en conjunto de test...\")\n",
    "\n",
    "test_loader = create_dataloader(\n",
    "    test_seqs, vocab, seq_len=seq_len, batch_size=batch_size, \n",
    "    pad_idx=pad_idx, sos_idx=sos_idx, eos_idx=eos_idx, shuffle=False\n",
    ")\n",
    "\n",
    "# Calcular métricas\n",
    "test_loss, test_acc, test_top5, test_mrr, test_perplexity = evaluate_model(\n",
    "    model, test_loader, criterion, device, top_k=top_k\n",
    ")\n",
    "\n",
    "print(f\"\\nResultados en Test:\")\n",
    "print(f\"  Loss: {test_loss:.4f}\")\n",
    "print(f\"  Top-1 Accuracy: {test_acc:.4f}\")\n",
    "print(f\"  Top-5 Accuracy: {test_top5:.4f}\")\n",
    "print(f\"  MRR: {test_mrr:.4f}\")\n",
    "print(f\"  Perplexity: {test_perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510af377",
   "metadata": {
    "id": "510af377"
   },
   "source": [
    "## 11) predict_next(context, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165bfb26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1756231161525,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "165bfb26",
    "outputId": "dba220e2-4839-4335-8d45-7af91a5344ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: ['III', 'III', '#IV', '#IV', 'bII', 'bII', 'natIII', 'natIII', 'III', '#IV', 'IV', 'VI', 'V', 'bVII', 'natVI', 'I', 'II', 'VII', 'VI', 'IV', 'III', 'I', 'VII', 'natVI']\n",
      "Pred: [('II', 0.24083514511585236), ('VI', 0.11550020426511765), ('VII', 0.09397098422050476), ('natIII', 0.05708475410938263), ('natVI', 0.05385180190205574)]\n"
     ]
    }
   ],
   "source": [
    "# Configuración de guardado\n",
    "save_dir = \"/content/models_lstm_v3\"        # Directorio para guardar los modelos\n",
    "model_filename = \"lstm_v3_final.pt\"        # Nombre del archivo del modelo\n",
    "tokenizer_filename = \"lstm_tokenizer.json\" # Nombre del archivo del tokenizer\n",
    "\n",
    "# Crear directorio si no existe\n",
    "import os\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model_path = os.path.join(save_dir, model_filename)\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'vocab_size': vocab_size,\n",
    "    'embed_dim': embed_dim,\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'num_layers': num_layers,\n",
    "    'dropout': dropout,\n",
    "    'seq_len': seq_len,\n",
    "    'test_acc': test_acc,\n",
    "    'test_mrr': test_mrr,\n",
    "    'test_perplexity': test_perplexity\n",
    "}, model_path)\n",
    "\n",
    "# Guardar el tokenizer\n",
    "tokenizer_path = os.path.join(save_dir, tokenizer_filename)\n",
    "with open(tokenizer_path, 'w') as f:\n",
    "    import json\n",
    "    # Convertir vocabulario a diccionario serializable\n",
    "    vocab_dict = {token: idx for token, idx in vocab.get_stoi().items()}\n",
    "    json.dump({\n",
    "        'vocab': vocab_dict,\n",
    "        'default_index': vocab.get_default_index()\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"Modelo guardado en: {model_path}\")\n",
    "print(f\"Tokenizer guardado en: {tokenizer_path}\")\n",
    "print(f\"Métricas del modelo:\")\n",
    "print(f\"  - Top-1 Accuracy: {test_acc:.4f}\")\n",
    "print(f\"  - MRR: {test_mrr:.4f}\")\n",
    "print(f\"  - Perplexity: {test_perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cn_1P6r3d2",
   "metadata": {
    "id": "e8cn_1P6r3d2"
   },
   "source": [
    "## 12) Inferencia incremental (quick test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jQ2Jh9KKrnti",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1756231161526,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "jQ2Jh9KKrnti"
   },
   "outputs": [],
   "source": [
    "# Configuración para ejemplos de predicción\n",
    "num_examples = 3      # Número de ejemplos a mostrar\n",
    "max_predictions = 5   # Máximo número de predicciones por ejemplo\n",
    "\n",
    "# Función para mostrar ejemplos de predicción\n",
    "def show_prediction_examples(model, test_loader, vocab, num_examples=3, max_predictions=5):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Obtener algunos ejemplos del test loader\n",
    "    examples_shown = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_targets in test_loader:\n",
    "            if examples_shown >= num_examples:\n",
    "                break\n",
    "                \n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "            \n",
    "            # Obtener predicciones\n",
    "            outputs = model(batch_inputs)\n",
    "            \n",
    "            # Procesar cada ejemplo en el batch\n",
    "            for i in range(min(batch_inputs.size(0), num_examples - examples_shown)):\n",
    "                input_seq = batch_inputs[i]\n",
    "                target_seq = batch_targets[i]\n",
    "                pred_logits = outputs[i]\n",
    "                \n",
    "                # Convertir secuencias a tokens\n",
    "                input_tokens = [vocab.get_itos()[idx.item()] for idx in input_seq if idx.item() != vocab[\"[PAD]\"]]\n",
    "                target_tokens = [vocab.get_itos()[idx.item()] for idx in target_seq if idx.item() != vocab[\"[PAD]\"]]\n",
    "                \n",
    "                print(f\"\\n--- Ejemplo {examples_shown + 1} ---\")\n",
    "                print(f\"Entrada: {' → '.join(input_tokens)}\")\n",
    "                print(f\"Target:  {' → '.join(target_tokens)}\")\n",
    "                \n",
    "                # Mostrar las top-k predicciones para cada posición\n",
    "                for pos in range(min(len(target_tokens), max_predictions)):\n",
    "                    if pos < pred_logits.size(0):\n",
    "                        probs = torch.softmax(pred_logits[pos], dim=0)\n",
    "                        top_k_probs, top_k_indices = torch.topk(probs, k=min(5, len(vocab)))\n",
    "                        \n",
    "                        pred_tokens = [vocab.get_itos()[idx.item()] for idx in top_k_indices]\n",
    "                        pred_probs = [prob.item() for prob in top_k_probs]\n",
    "                        \n",
    "                        print(f\"Pos {pos+1}: {target_tokens[pos] if pos < len(target_tokens) else '[PAD]'}\")\n",
    "                        for j, (token, prob) in enumerate(zip(pred_tokens, pred_probs)):\n",
    "                            marker = \"✓\" if token == target_tokens[pos] else \" \"\n",
    "                            print(f\"  {marker} {j+1}. {token}: {prob:.3f}\")\n",
    "                \n",
    "                examples_shown += 1\n",
    "                if examples_shown >= num_examples:\n",
    "                    break\n",
    "\n",
    "# Mostrar ejemplos de predicción\n",
    "print(\"Ejemplos de predicción del modelo LSTM:\")\n",
    "show_prediction_examples(model, test_loader, vocab, num_examples, max_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "S1nHo4t3gwY9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1756231161560,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "S1nHo4t3gwY9",
    "outputId": "1364eb97-dd90-440d-9e6e-3da200c5f5ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('V', 0.19276435673236847),\n",
       " ('V7', 0.1380607634782791),\n",
       " ('v', 0.06758282333612442),\n",
       " ('ii', 0.06204349175095558),\n",
       " ('bVII', 0.049799397587776184)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = StatefulPredictor(model, stoi, itos, device)\n",
    "sp.suggest([\"ii\",\"V\"], k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "__5FJ5ZnXwkC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 130,
     "status": "ok",
     "timestamp": 1756231161688,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "__5FJ5ZnXwkC",
    "outputId": "06f99e36-e389-4ad8-896d-d1b50c7aa39f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('V7', 0.27057090401649475),\n",
       " ('i', 0.12024112790822983),\n",
       " ('vi', 0.11982262134552002),\n",
       " ('ii', 0.10807666927576065),\n",
       " ('viiø', 0.05500981584191322)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.suggest([\"i\",\"vi\", 'ii'], k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "qfVOVw4JYSJN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114,
     "status": "ok",
     "timestamp": 1756231161689,
     "user": {
      "displayName": "Antonio L. Trapote",
      "userId": "06525061061648817839"
     },
     "user_tz": -120
    },
    "id": "qfVOVw4JYSJN",
    "outputId": "b0bfd39e-bcca-45d5-f878-bc440d328e5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('VI', 0.09826728701591492),\n",
       " ('bII7', 0.09121581166982651),\n",
       " ('i', 0.08353113383054733),\n",
       " ('bii', 0.07816524803638458),\n",
       " ('viø', 0.06509044766426086)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.suggest([\"i\",\"bvi\"], k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53afbbdb",
   "metadata": {
    "id": "53afbbdb"
   },
   "source": [
    "### Roadmap\n",
    "- Ajuste de hiperparámetros (Ranadom Search)\n",
    "- Re-ranking suave para evitar repes y favorecer cadencias.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
